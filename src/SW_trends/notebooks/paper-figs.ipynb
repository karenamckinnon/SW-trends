{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed7937a7",
   "metadata": {},
   "source": [
    "# Figures assessing historical trends in shortwave radiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5f8ab6eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "# Load my packages\n",
    "import SW_trends.utils as my_utils\n",
    "from helpful_utilities.ncutils import lon_to_360\n",
    "import helpful_utilities.xutils as my_xutils\n",
    "import helpful_utilities.stats as my_stat_utils\n",
    "from helpful_utilities.plotting import plot_global_discrete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6aa8eec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load standard packages for analysis\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "from scipy.stats import linregress, gaussian_kde\n",
    "import numpy as np\n",
    "\n",
    "# Load packages for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "import matplotlib.colors as mcolors\n",
    "from matplotlib.colors import BoundaryNorm\n",
    "from matplotlib import gridspec\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "\n",
    "# Load packages for file management and other misc\n",
    "from glob import glob\n",
    "import cftime\n",
    "import os\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ad989669",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory information (update to use this on other systems)\n",
    "figdir = '/home/kmckinnon/SW-trends/figs'\n",
    "procdir = '/home/data/projects/SW-trends/proc'\n",
    "\n",
    "geba_dir = '/home/data/GEBA'\n",
    "\n",
    "data_dirs = {\n",
    "    'JRA3Q': '/home/data/JRA3Q/monthly',\n",
    "    'CESM2': '/home/data/CESM2/LE/monthly',\n",
    "    'GOGA': '/home/data/CESM2/GOGA/monthly',\n",
    "    'CLARA': '/home/data/CMSAF/CLARA',\n",
    "    'ERA5': '/home/data/ERA5/month',\n",
    "    'GEWEX': '/home/data/GEWEX/Shortwave_monthly_utc_1',\n",
    "    'CERES': '/home/data/CERES',\n",
    "    'MERRA2': '/home/data/MERRA2/monthly/SW_down',\n",
    "}\n",
    "\n",
    "# Variable names by product\n",
    "name_dict = {\n",
    "    'MERRA2': 'SWGDN',\n",
    "    'GEWEX': 'all_sw_dn_sfc',\n",
    "    'CLARA': 'SIS',\n",
    "    'ERA5': 'surface_solar_radiation_downwards',\n",
    "    'CERES': 'sfc_sw_down_all_mon',\n",
    "    'CESM2': 'FSDS',\n",
    "    'GOGA': 'FSDS',\n",
    "    'JRA3Q': 'dswrf1have-sfc-fc-gauss-mn'\n",
    "}\n",
    "\n",
    "reanalysis_names = 'ERA5', 'MERRA2', 'JRA3Q', 'CLARA', 'GEWEX' # compare to reanalysis and other RS products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7441df3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "years_to_use = 1980, 2024\n",
    "nyrs = years_to_use[1] - years_to_use[0] + 1\n",
    "\n",
    "var_to_analyze = 'Downward surface shortwave'\n",
    "short_name = {'Downward surface shortwave': 'SW_down'}\n",
    "this_var = short_name[var_to_analyze]\n",
    "\n",
    "letters = list(string.ascii_lowercase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aeb140ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trend colorbar\n",
    "vmin, vmax = -0.5, 0.5\n",
    "step = 0.1\n",
    "levels = np.arange(vmin, vmax + step, step)\n",
    "norm = BoundaryNorm(levels, ncolors=256)\n",
    "cmap = 'RdBu_r'\n",
    "\n",
    "trend_cbar = {'levels': levels,\n",
    "              'norm': norm,\n",
    "              'cmap': cmap}\n",
    "\n",
    "# Correlation colorbar\n",
    "vmin, vmax = -1, 1\n",
    "step = 0.2\n",
    "levels = np.arange(vmin, vmax + step, step)\n",
    "norm = BoundaryNorm(levels, ncolors=256)\n",
    "cmap = 'RdBu_r'\n",
    "\n",
    "corr_cbar = {'levels': levels,\n",
    "              'norm': norm,\n",
    "              'cmap': cmap}\n",
    "\n",
    "# Positive correlation colorbar\n",
    "vmin, vmax = 0, 1\n",
    "step = 0.1\n",
    "levels = np.arange(vmin, vmax + step, step)\n",
    "norm = BoundaryNorm(levels, ncolors=256)\n",
    "cmap = 'Reds'\n",
    "\n",
    "pos_corr_cbar = {'levels': levels,\n",
    "                 'norm': norm,\n",
    "                 'cmap': cmap}\n",
    "\n",
    "trend_str = 'SW$_\\downarrow$ trend (W/m$^2$/year)'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72536a24",
   "metadata": {},
   "source": [
    "# Load land masks\n",
    "- also remove Greenland, and subset to -60, 80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a51113f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1x1\n",
    "f_lsmask_1x1 = '/home/data/ERA5/fx/era5_lsmask_1x1.nc'\n",
    "da_lsmask_1x1 = xr.open_dataarray(f_lsmask_1x1)\n",
    "analysis_mask = my_utils.get_analysis_mask(da_lsmask_1x1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "629896ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For regridding needs\n",
    "shared_lats = analysis_mask.lat.values\n",
    "shared_lons = analysis_mask.lon.values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7410a071",
   "metadata": {},
   "source": [
    "# Load all gridded data (satellite products, reanalyses, models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0c9cd2e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MERRA2, SWGDN\n",
      "GEWEX, all_sw_dn_sfc\n",
      "CLARA, SIS\n",
      "ERA5, surface_solar_radiation_downwards\n",
      "CERES, sfc_sw_down_all_mon\n",
      "CESM2, FSDS\n",
      "GOGA, FSDS\n",
      "JRA3Q, dswrf1have-sfc-fc-gauss-mn\n"
     ]
    }
   ],
   "source": [
    "all_sw = []\n",
    "all_sw_native = []\n",
    "name_list = []\n",
    "name_list_native = []\n",
    "\n",
    "names = name_dict.keys()\n",
    "for name in names:\n",
    "\n",
    "\n",
    "    this_dir = data_dirs[name]\n",
    "    v = name_dict[name]\n",
    "    print('%s, %s' % (name, v))\n",
    "    if (name == 'CESM2') | (name == 'GOGA') | (name == 'ERA5') | (name == 'JRA3Q'):\n",
    "        this_dir = this_dir + '/%s' % v\n",
    "\n",
    "    if (name == 'CESM2') | (name == 'GOGA'):  # Ensembles\n",
    "        if (name == 'CESM2'):\n",
    "            # Load CESM2\n",
    "            files = sorted(glob('%s/b.e21.B*smbb.f09_g17.*.*.h0.%s.??????-??????.nc' % \n",
    "                                (this_dir, v)))\n",
    "            ens_members = np.unique(np.array(['.'.join(f.split('.')[4:6]) for f in files]))\n",
    "        else:\n",
    "            # Load GOGA\n",
    "            files = sorted(glob('%s/f.e21.F*.f09_f09.*goga*.cam.h0.%s*.??????-??????.nc' % \n",
    "                                (this_dir, v)))\n",
    "            ens_members = ['%02i' % n for n in range(1, 11)]\n",
    "\n",
    "        files = [\n",
    "            f for f in files\n",
    "            if (years := my_utils.extract_years(f)) and (years[0] <= (years_to_use[-1] + 1) \n",
    "                                                         and years[1] >= (years_to_use[0] + 1))\n",
    "        ]\n",
    "\n",
    "        da_sw = []\n",
    "        \n",
    "        # Load each ensemble member\n",
    "        for ens_member in ens_members:\n",
    "            use_files = [f for f in files if ens_member in f]\n",
    "            da = xr.open_mfdataset(use_files)[v]\n",
    "            \n",
    "            # Round lat/lon since they can slightly differ between CESM products\n",
    "            da['lat'] = np.round(da['lat'], 3).data.astype('float32')\n",
    "            da['lon'] = np.round(da['lon'], 3).data.astype('float32')\n",
    "            da_sw.append(da)\n",
    "            \n",
    "        da_sw = xr.concat(da_sw, dim='member')\n",
    "        da_sw['member'] = ens_members\n",
    "        da_sw = da_sw.rename(v)\n",
    "\n",
    "        # Move time by one month because of CESM2 timestamp issues for monthly data\n",
    "        # (the monthly averages are saved with the next month's time)\n",
    "        new_time = [cftime.DatetimeNoLeap(t.year, t.month - 1, t.day) if t.month > 1 \n",
    "                    else cftime.DatetimeNoLeap(t.year - 1, 12, t.day) \n",
    "                    for t in da_sw['time'].values]\n",
    "        da_sw['time'] = new_time\n",
    "\n",
    "    else:\n",
    "        varname = name_dict[name]\n",
    "        files = sorted(glob('%s/*.nc' % this_dir))\n",
    "        if name == 'GEWEX':\n",
    "            files = sorted(glob('%s/*.nc4' % this_dir))\n",
    "        if name == 'ERA5':\n",
    "            da_sw = xr.open_dataarray(files[0])  # ERA5 has single file\n",
    "        else:\n",
    "            da_sw = xr.open_mfdataset(files)[varname]\n",
    "\n",
    "        if name == 'ERA5':\n",
    "            da_sw = da_sw.rename({'valid_time': 'time'})\n",
    "            adjusted_time = da_sw.time.dt.floor('D')\n",
    "            da_sw['time'] = adjusted_time\n",
    "            # Heating is in J/m2\n",
    "            sec_per_day = 3600 * 24\n",
    "            da_sw /= sec_per_day\n",
    "\n",
    "        if name == 'LANDFLUX':  # mask to max total obs - some gridboxes on the edge have few measurements\n",
    "            max_obs = (~np.isnan(da_sw)).sum('time').max()\n",
    "            has_max = (~np.isnan(da_sw)).sum('time') == max_obs\n",
    "            da_sw = da_sw.where(has_max)\n",
    "\n",
    "    # Book-keeping: all to lat/lon, lon is 0-360, consistent names, same time period\n",
    "    if 'latitude' in da_sw.dims:\n",
    "        da_sw = da_sw.rename({'latitude': 'lat', 'longitude': 'lon'})\n",
    "    da_sw = da_sw.sortby('lat').rename('%s_%s' % (name, short_name[var_to_analyze]))\n",
    "    da_sw = lon_to_360(da_sw)\n",
    "    \n",
    "    # For reanalysis, for comparion to station data, keep native resolution as well\n",
    "    if (name == 'ERA5') | (name == 'JRA3Q') | (name == 'MERRA2') | (name == 'CLARA') | (name == 'GEWEX'):\n",
    "        all_sw_native.append(da_sw.copy())\n",
    "        name_list_native.append(name)\n",
    "        \n",
    "    da_interp = my_utils.regrid_to_shared_grid(da_sw, shared_lats, shared_lons)\n",
    "    \n",
    "    name_list.append('%s_%s' % (name, short_name[var_to_analyze]))\n",
    "    all_sw.append(da_interp.sel(time=slice('%04i' % years_to_use[0], \n",
    "                                           '%04i' % years_to_use[1])).load())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1967c82",
   "metadata": {},
   "source": [
    "# Compute annual trends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a702ac5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MERRA2_SW_down\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'dfg' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 15\u001b[0m\n\u001b[1;32m     13\u001b[0m ann_mean \u001b[38;5;241m=\u001b[39m ann_mean\u001b[38;5;241m.\u001b[39msel(year\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mslice\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%04i\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m years_to_use[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%04i\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m years_to_use[\u001b[38;5;241m1\u001b[39m]))\n\u001b[1;32m     14\u001b[0m trend \u001b[38;5;241m=\u001b[39m my_xutils\u001b[38;5;241m.\u001b[39mcompute_linear_trend_per_year(ann_mean)\n\u001b[0;32m---> 15\u001b[0m \u001b[43mdfg\u001b[49m\n\u001b[1;32m     17\u001b[0m trend_maps\u001b[38;5;241m.\u001b[39mappend(trend)\n\u001b[1;32m     18\u001b[0m start_year\u001b[38;5;241m.\u001b[39mappend(ann_mean\u001b[38;5;241m.\u001b[39myear[\u001b[38;5;241m0\u001b[39m])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dfg' is not defined"
     ]
    }
   ],
   "source": [
    "trend_maps = []\n",
    "start_year = []\n",
    "end_year = []\n",
    "\n",
    "for ct, da in enumerate(all_sw):\n",
    "\n",
    "    print(da.name)\n",
    "        \n",
    "    # remove seasonal cycle to avoid some issues with preferential sampling across specific months\n",
    "    # for datasets without complete coverage\n",
    "    da_anom = da.groupby('time.month') - da.groupby('time.month').mean()\n",
    "    ann_mean = my_xutils.compute_annual_mean_of_full_years(da_anom)\n",
    "    ann_mean = ann_mean.sel(year=slice('%04i' % years_to_use[0], '%04i' % years_to_use[1]))\n",
    "    trend = my_xutils.compute_linear_trend_per_year(ann_mean)\n",
    "\n",
    "    trend_maps.append(trend)\n",
    "    start_year.append(ann_mean.year[0])\n",
    "    end_year.append(ann_mean.year[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "853050ab",
   "metadata": {},
   "source": [
    "# Load auxiliary datesets (ISCCP, CERES TOA, MERRA2 AOD, ERA5 clouds, ERA5 TOA )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20f03611",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get ISCCP data\n",
    "isccp_dir = '/home/data/ISCCP/BASIC'\n",
    "files = sorted(glob('%s/*.nc' % isccp_dir))\n",
    "ds_isccp = xr.open_mfdataset(files)\n",
    "da_cldamt_isccp = ds_isccp['cldamt_irtypes'].load()\n",
    "del ds_isccp\n",
    "\n",
    "# Get TOA CERES data\n",
    "f_ceres_toa = '/home/data/CERES/CERES_EBAF-TOA_Ed4.2_Subset_200003-202407.nc'\n",
    "ds_ceres_toa = xr.open_dataset(f_ceres_toa)\n",
    "da_ceres_toa = ds_ceres_toa['solar_mon']\n",
    "del ds_ceres_toa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e96f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get MERRA2 AOD data\n",
    "aod_files = sorted(glob('/home/data/MERRA2/monthly/AOD/*.nc'))\n",
    "da_aod = xr.open_mfdataset(aod_files)['AODANA']\n",
    "da_aod = ((lon_to_360(da_aod.sortby('lat'))).interp(lat=shared_lats, lon=shared_lons)).load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72acb68d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get ERA5 cloud data\n",
    "savename = '%s/ds_clouds_era5_coarse.nc' % procdir\n",
    "if os.path.isfile(savename):\n",
    "    ds_clouds_era5_coarse = xr.open_dataset(savename)\n",
    "else:\n",
    "    era5_cloud_names = ['%s_cloud_cover' % c for c in list(('low', 'medium', 'high', 'total'))]\n",
    "    ds_clouds_era5 = []\n",
    "    for var in era5_cloud_names:\n",
    "        this_dir = '%s/%s' % (data_dirs['ERA5'], var)\n",
    "        da = xr.open_dataarray('%s/%s.nc' % (this_dir, var))\n",
    "        da = da.rename({'valid_time': 'time'})\n",
    "        adjusted_time = da.time.dt.floor('D')\n",
    "        da['time'] = adjusted_time\n",
    "        da = da.rename({'latitude': 'lat', 'longitude': 'lon'})\n",
    "        da = da.sortby('lat').rename(var)\n",
    "        da = da.sel(time=slice('%04i' % years_to_use[0], \n",
    "                               '%04i' % years_to_use[1]))\n",
    "        ds_clouds_era5.append(da.load())\n",
    "    ds_clouds_era5 = xr.merge(ds_clouds_era5)\n",
    "    ds_clouds_era5_coarse = ds_clouds_era5.interp(lat=shared_lats, lon=shared_lons)\n",
    "    del ds_clouds_era5\n",
    "    ds_clouds_era5_coarse.to_netcdf(savename)\n",
    "\n",
    "# Load ERA5 TOA\n",
    "savename = '%s/da_era5_toa_coarse.nc' % procdir\n",
    "if os.path.isfile(savename):\n",
    "    da_era5_toa_coarse = xr.open_dataset(savename)\n",
    "else:\n",
    "    toa_var = 'toa_incident_solar_radiation'\n",
    "    this_dir = '%s/%s' % (data_dirs['ERA5'], toa_var)\n",
    "    da_era5_toa = xr.open_dataarray('%s/%s.nc' % (this_dir, toa_var))\n",
    "    da_era5_toa = da_era5_toa.rename({'valid_time': 'time'}).drop('expver')\n",
    "    adjusted_time = da_era5_toa.time.dt.floor('D')\n",
    "    da_era5_toa['time'] = adjusted_time\n",
    "    da_era5_toa = da_era5_toa.rename({'latitude': 'lat', 'longitude': 'lon'})\n",
    "    da_era5_toa = da_era5_toa.sortby('lat').rename(toa_var)\n",
    "    da_era5_toa = da_era5_toa.sel(time=slice('%04i' % years_to_use[0], '%04i' % years_to_use[1]))\n",
    "    da_era5_toa_coarse = da_era5_toa.interp(lat=shared_lats, lon=shared_lons)\n",
    "    del da_era5_toa\n",
    "    da_era5_toa_coarse.to_netcdf(savename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d7254c0",
   "metadata": {},
   "source": [
    "# Reanalysis validation\n",
    "1. CERES interannual variability\n",
    "2. CERES trends\n",
    "3. GEBA interannual (full period and pre-CERES)\n",
    "4. GEBA trends"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b8b99a2",
   "metadata": {},
   "source": [
    "## CERES metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eace7f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_fdr = 0.01\n",
    "\n",
    "idx_CERES = np.where(['CERES_%s' % (this_var) == n for n in name_list])[0][0]\n",
    "ceres_start_year = start_year[idx_CERES]\n",
    "ceres_end_year = end_year[idx_CERES]\n",
    "ceres_ann = my_xutils.compute_annual_mean_of_full_years(all_sw[idx_CERES])\n",
    "for rname in reanalysis_names:\n",
    "    print(rname)\n",
    "    \n",
    "    savename = '%s/validation_metrics_%s.nc' % (procdir, rname)\n",
    "    if os.path.isfile(savename):\n",
    "        continue\n",
    "    \n",
    "    idx_r = np.where(['%s_%s' % (rname, this_var) == n for n in name_list])[0][0]\n",
    "    \n",
    "    # (1) correlation with CERES\n",
    "    \n",
    "    # Calculate annual mean of reanalysis\n",
    "    sub_re = all_sw[idx_r].sel(time=slice('%04i' % ceres_start_year, '%04i' % ceres_end_year))\n",
    "    reanalysis_ann = my_xutils.compute_annual_mean_of_full_years(sub_re)\n",
    "    # Subset both to shared period\n",
    "    this_start = np.max((ceres_start_year, start_year[idx_r]))\n",
    "    this_end = np.min((ceres_end_year, end_year[idx_r]))\n",
    "    \n",
    "    reanalysis_ann = reanalysis_ann.sel(year=slice('%04i' % this_start, '%04i' % this_end))\n",
    "    \n",
    "    # Calculate correlation\n",
    "    rho, pvals = my_xutils.pearsonr_xr(reanalysis_ann.where(analysis_mask), \n",
    "                                       ceres_ann.where(analysis_mask).sel(year=slice('%04i' % this_start, \n",
    "                                                                                     '%04i' % this_end)), \n",
    "                                       dim='year')\n",
    "    \n",
    "    # Mask part 1: cases where there is not a significant correlation\n",
    "    is_sig_map = my_stat_utils.fdr_da(pvals.where(analysis_mask), alpha_fdr=alpha_fdr)\n",
    "    \n",
    "    # (2) Assess whether the trends are different via significance of difference in datasets\n",
    "    delta_da = reanalysis_ann - ceres_ann.sel(year=slice('%04i' % this_start, '%04i' % this_end))\n",
    "    delta_da = delta_da.where(analysis_mask)\n",
    "    _, delta_pval = my_xutils.xr_linregress_pval(delta_da)\n",
    "\n",
    "    is_sig_map_trend_diff = my_stat_utils.fdr_da(delta_pval, alpha_fdr=alpha_fdr)\n",
    "    \n",
    "    r_quality_mask = (is_sig_map == 1).astype(int) & (is_sig_map_trend_diff == 0).astype(int)\n",
    "    r_quality_mask = r_quality_mask.where(analysis_mask)\n",
    "    \n",
    "    # Get trend for subset period to compare to CERES\n",
    "    r_trend_sub, _ = my_xutils.xr_linregress_pval(reanalysis_ann)\n",
    "    \n",
    "    ds_valid = xr.merge((is_sig_map.rename('sig_corr'),\n",
    "                         is_sig_map_trend_diff.rename('sig_diff_trends'),\n",
    "                         r_quality_mask.rename('quality_mask'), \n",
    "                         rho.rename('correlation'),\n",
    "                         r_trend_sub.rename('CERES_era_trend')))\n",
    "    ds_valid.to_netcdf(savename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe16f665",
   "metadata": {},
   "source": [
    "## GEBA metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da2a949f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First load station data from GEBA (annual mean)\n",
    "files = glob('%s/*.csv' % geba_dir)\n",
    "md = pd.read_csv(files[0])\n",
    "geba_ann = pd.read_csv(files[-1])   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "603763ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Will need to pull native reanalysis grids\n",
    "all_sw_native_loaded = []\n",
    "for r in reanalysis_names:\n",
    "    idx_r = np.where([r == n for n in name_list_native])[0][0]\n",
    "    all_sw_native_loaded.append(all_sw_native[idx_r].load())\n",
    "#del all_sw_native"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eca8186",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Pull two different sets of GEBA data:\n",
    "# 0: 1980-2024, has at least 20 years of data\n",
    "# 1: 1980-2000, has at least 10 years of data\n",
    "savename_trends = '%s/geba_reanalysis_with_RS_trends.npz' % procdir\n",
    "\n",
    "if not os.path.isfile(savename_trends):\n",
    "    for geba_ct in range(2):\n",
    "\n",
    "        # Define time thresholds\n",
    "        if geba_ct == 0:  # Full period\n",
    "            cutoff_year1 = 1980\n",
    "            cutoff_year2 = 2024\n",
    "            min_years = 20\n",
    "            savestr = 'full_record_%iyearsmin' % min_years\n",
    "\n",
    "        else:  # Before CERES but during reanalysis\n",
    "            cutoff_year1 = 1980\n",
    "            cutoff_year2 = 2000\n",
    "            min_years = 10\n",
    "            savestr = 'pre_ceres_%iyearsmin' % min_years\n",
    "\n",
    "        savename_interannual = '%s/geba_reanalysis_with_RS_corr_%s.npz' % (procdir, savestr)\n",
    "        print(savename_interannual)\n",
    "\n",
    "        # Group by station\n",
    "        grouped = geba_ann.groupby('tskey')\n",
    "\n",
    "        valid_tskeys = []\n",
    "\n",
    "        for tskey, group in grouped:\n",
    "            # check that it is direct\n",
    "            this_type = md.loc[md['tskey'] == tskey, 'ebcode']\n",
    "            if this_type.values[0] != 'GLOBAL':\n",
    "                continue\n",
    "            group = group.dropna(subset=['converted_flux_avg'])\n",
    "\n",
    "            # Subset to specific period\n",
    "            subset = group.loc[(group['year'] >= cutoff_year1) & (group['year'] < cutoff_year2)]\n",
    "\n",
    "            # Count obs\n",
    "            N = (~np.isnan(subset['converted_flux_avg'])).sum()\n",
    "            has_enough_data = N >= min_years\n",
    "\n",
    "            if has_enough_data:\n",
    "                valid_tskeys.append(tskey)\n",
    "\n",
    "        # Final list of tskeys\n",
    "        valid_tskeys = sorted(valid_tskeys)\n",
    "        md_valid = md[md['tskey'].isin(valid_tskeys)]\n",
    "        N = len(md_valid)\n",
    "        geba_lats = md_valid['sgylat']\n",
    "        geba_lons = md_valid['sgxlon']\n",
    "\n",
    "        if geba_ct == 0:\n",
    "            trends_save = np.nan * np.ones((len(reanalysis_names) + 2, N))\n",
    "        rho_geba_reanalysis = np.nan * np.ones((len(reanalysis_names), N))\n",
    "\n",
    "        for ct, key in enumerate(valid_tskeys):\n",
    "            if (ct % 10) == 0:\n",
    "                print('%i/%i' % (ct, len(valid_tskeys)))\n",
    "            this_ts = geba_ann.loc[geba_ann['tskey'] == key]\n",
    "            this_lat = md_valid.loc[md_valid['tskey'] == key, 'sgylat'].values[0]\n",
    "            this_lon = md_valid.loc[md_valid['tskey'] == key, 'sgxlon'].values[0]\n",
    "\n",
    "            # remove flagged and outlier values\n",
    "            is_ok = (this_ts['computed_flag_avg'] == 8).values\n",
    "            years = this_ts['year'][is_ok]\n",
    "            vals = this_ts['converted_flux_avg'][is_ok]\n",
    "\n",
    "            # remove outliers beyond 4sigma\n",
    "            outlier = ((vals >= (4 * np.std(vals) + np.mean(vals))) | \n",
    "                       (vals <= (-4 * np.std(vals) + np.mean(vals))))\n",
    "#             if np.sum(outlier) > 0:\n",
    "#                 fig, ax = plt.subplots(figsize=(5, 2))\n",
    "#                 ax.plot(years, vals, 'ok')\n",
    "#                 ax.plot(years[outlier], vals[outlier], 'sr')\n",
    "#                 plt.show()\n",
    "            years = years[~outlier]\n",
    "            vals = vals[~outlier]\n",
    "\n",
    "            # subset to reanalysis period\n",
    "            reanalysis_period = (years >= years_to_use[0]) & (years <= years_to_use[1])\n",
    "            years = np.array(years[reanalysis_period])  # GEBA years subset to reanalysis period\n",
    "            vals = np.array(vals[reanalysis_period])\n",
    "\n",
    "            if geba_ct == 0:  # trends during longer GEBA periods\n",
    "                # remove mean during matching years\n",
    "                sub_geba = vals.copy()\n",
    "                mu_geba = np.mean(sub_geba)\n",
    "                sub_geba_anom = sub_geba - mu_geba\n",
    "                shared_years = np.intersect1d(years, np.arange(1980, 2025))\n",
    "                X = shared_years - np.mean(shared_years)\n",
    "                slope_geba, intercept_geba, r_value, _, _ = linregress(X, sub_geba_anom)\n",
    "                trends_save[0, ct] = slope_geba\n",
    "\n",
    "            for o_ct, rname in enumerate(reanalysis_names):\n",
    "                \n",
    "                # Get reanalysis\n",
    "                this_ts_reanalysis = my_xutils.compute_annual_mean_of_full_years(\n",
    "                    all_sw_native_loaded[o_ct].\n",
    "                    sel(lat=this_lat, lon=this_lon, method='nearest')).squeeze()\n",
    "                \n",
    "                # Match reanalysis to GEBA\n",
    "                shared_years = np.intersect1d(years, this_ts_reanalysis.year)\n",
    "                X = shared_years - np.mean(shared_years)\n",
    "                sub_reanalysis = this_ts_reanalysis.sel(year=shared_years)\n",
    "                \n",
    "                # Get idx to match GEBA to reanalysis (relevant for GEWEX only)\n",
    "                idx_match = np.isin(years, sub_reanalysis.year)\n",
    "                assert(years[idx_match] == sub_reanalysis.year).all()\n",
    "                \n",
    "                if (rname == 'GEWEX') & (geba_ct == 0):  # need to calculate trends using shorter period\n",
    "\n",
    "                    sub_geba = vals[idx_match]\n",
    "                    mu_geba = np.mean(sub_geba)\n",
    "                    sub_geba_anom = sub_geba - mu_geba\n",
    "                    slope_geba, intercept_geba, r_value, _, _ = linregress(X, sub_geba_anom)\n",
    "                    trends_save[-1, ct] = slope_geba\n",
    "                \n",
    "                mu_reanalysis = sub_reanalysis.mean('year')\n",
    "                sub_reanalysis_anom = sub_reanalysis - mu_reanalysis\n",
    "\n",
    "                if geba_ct == 0:\n",
    "                    slope_reanalysis, intercept_reanalysis, _, _, _ = linregress(X, sub_reanalysis_anom)\n",
    "                    trends_save[o_ct + 1, ct] = slope_reanalysis\n",
    "\n",
    "                # For all sets, calculate correlation\n",
    "                rho_geba_reanalysis[o_ct, ct] = np.corrcoef(sub_reanalysis_anom.values, \n",
    "                                                            vals[idx_match])[0, 1]\n",
    "\n",
    "        # Save!\n",
    "        if geba_ct == 0:\n",
    "            np.savez(savename_trends,\n",
    "                     lats=geba_lats, lons=geba_lons, trends_save=trends_save)\n",
    "        np.savez(savename_interannual, \n",
    "                 lats=geba_lats, lons=geba_lons, rho_geba_reanalysis=rho_geba_reanalysis)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d648b3b7",
   "metadata": {},
   "source": [
    "## Figure 1 (ERA5) and Supplemental figures (JRA3Q, MERRA2, CLARA, GEWEX)\n",
    "\n",
    "6 maps: \n",
    "- first row: interannual variability (one colorbar)\n",
    "- second row: trends in CERES and ERA5 (second colorbar)\n",
    "- third row: trends in GEBA and ERA5 (shared with second row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2655423a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "geba_savename_trends = '%s/geba_reanalysis_with_RS_trends.npz' % procdir\n",
    "geba_savename_interannual = '%s/geba_reanalysis_with_RS_corr_full_record_20yearsmin.npz' % procdir\n",
    "\n",
    "for r_ct, rname in enumerate(reanalysis_names):\n",
    "    \n",
    "    figname = '%s/validate_%s.png' % (figdir, rname)\n",
    "\n",
    "    ceres_savename = '%s/validation_metrics_%s.nc' % (procdir, rname)\n",
    "\n",
    "    # Get relevant fields\n",
    "    ds_ceres_validation = xr.open_dataset(ceres_savename)\n",
    "    ds_geba_trends = np.load(geba_savename_trends)\n",
    "    ds_geba_interannual = np.load(geba_savename_interannual)\n",
    "    \n",
    "    countries_for_plot = []\n",
    "    is_china = []\n",
    "    for this_lat, this_lon in zip(ds_geba_interannual['lats'], ds_geba_interannual['lons']):\n",
    "        country = md.loc[(md['sgxlon'] == this_lon) & (md['sgylat'] == this_lat), 'affiliation']\n",
    "        countries_for_plot.append(country.values[0])\n",
    "        is_china.append('China' in country.values[0])\n",
    "    \n",
    "    # print out metrics for paper\n",
    "   \n",
    "    r_avg = my_xutils.area_weighted_average(ds_ceres_validation['correlation'].\n",
    "                                            where(ds_ceres_validation['sig_corr']))\n",
    "    print('Area-weighted avg corr, %s-CERES: %0.2f' % (rname, r_avg.values))\n",
    "    \n",
    "    print('Median corr with GEBA: %0.2f' % np.median(ds_geba_interannual['rho_geba_reanalysis'][r_ct, :]))\n",
    "    corr_outside_china = ds_geba_interannual['rho_geba_reanalysis'][r_ct, ~(np.array(is_china).astype(bool))]\n",
    "    print('Median corr without China: %0.2f' % np.median(corr_outside_china))\n",
    "    corr_trends = np.corrcoef(ds_geba_trends['trends_save'][0, :], \n",
    "                              ds_geba_trends['trends_save'][r_ct + 1, :])[0, 1]\n",
    "    corr_trends_no_china = np.corrcoef(ds_geba_trends['trends_save'][0, ~(np.array(is_china).astype(bool))], \n",
    "                                       ds_geba_trends['trends_save'][r_ct + 1, \n",
    "                                                                     ~(np.array(is_china).astype(bool))])[0, 1]\n",
    "    bias_trends = (- np.mean(ds_geba_trends['trends_save'][0, :]) \n",
    "                   + np.mean(ds_geba_trends['trends_save'][r_ct + 1, :]))\n",
    "    print('correlation across stations of trends: %0.2f' % corr_trends)\n",
    "    print('correation without China: %0.2f' % corr_trends_no_china)\n",
    "    print('bias trends: %0.2f' % bias_trends)\n",
    "    \n",
    "    area_frac_mask = (my_xutils.area_weighted_average(ds_ceres_validation['quality_mask'] == 1) / \n",
    "                      my_xutils.area_weighted_average(~np.isnan(ds_ceres_validation['quality_mask'])))\n",
    "    print('Fraction of land area un-masked: %0.2f' % area_frac_mask)\n",
    "    print('******')\n",
    "\n",
    "    nrows = 3\n",
    "    ncols = 2\n",
    "    fig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=(6 * ncols, 3 * nrows),\n",
    "                             subplot_kw={'projection': ccrs.PlateCarree()},\n",
    "                             gridspec_kw={'hspace': 0.02, 'wspace': 0.02})  # adjust spacing)\n",
    "\n",
    "\n",
    "    fig_names = ['Corr(%s, CERES)' % rname,\n",
    "                 'Corr(%s, GEBA)' % rname,\n",
    "                 '%s trend subset to CERES period of record' % rname,\n",
    "                 'CERES trend',\n",
    "                 '%s trend subset to GEBA period of record' % rname,\n",
    "                 'GEBA trend']\n",
    "\n",
    "    text_ct = 0\n",
    "    for row_ct in range(nrows):\n",
    "        for col_ct in range(ncols):\n",
    "            # First row: interannual variability\n",
    "            if (row_ct == 0) & (col_ct == 0):\n",
    "                to_plot = ds_ceres_validation['correlation']\n",
    "                is_grid = True\n",
    "            elif (row_ct == 0) & (col_ct == 1):\n",
    "                to_plot = ds_geba_interannual['rho_geba_reanalysis'][r_ct, :]\n",
    "                this_lats = ds_geba_interannual['lats']\n",
    "                this_lons = ds_geba_interannual['lons']\n",
    "                is_grid = False\n",
    "            elif (row_ct == 1) & (col_ct == 0):\n",
    "                to_plot = ds_ceres_validation['CERES_era_trend']  # reanalysis trend during CERES era\n",
    "                is_grid = True\n",
    "            elif (row_ct == 1) & (col_ct == 1):\n",
    "                ceres_idx = np.where(['%s_%s' % ('CERES', this_var) == n for n in name_list])[0][0]\n",
    "                to_plot = trend_maps[ceres_idx]  # CERES trend\n",
    "\n",
    "                is_grid = True\n",
    "            elif (row_ct == 2) & (col_ct == 0):  # reanalysis trends overlapping with GEBA\n",
    "                to_plot = ds_geba_trends['trends_save'][r_ct + 1, :]\n",
    "                this_lats = ds_geba_trends['lats']\n",
    "                this_lons = ds_geba_trends['lons']\n",
    "                is_grid = False\n",
    "            elif (row_ct == 2) & (col_ct == 1):\n",
    "                if rname == 'GEWEX':\n",
    "                    to_plot = ds_geba_trends['trends_save'][-1, :]  # GEBA trends subset to GEWEX period\n",
    "                else:\n",
    "                    to_plot = ds_geba_trends['trends_save'][0, :]  # GEBA trends\n",
    "                this_lats = ds_geba_trends['lats']\n",
    "                this_lons = ds_geba_trends['lons']\n",
    "                is_grid = False\n",
    "\n",
    "            if row_ct == 0:\n",
    "                cbar_dict = pos_corr_cbar\n",
    "                to_hatch = ds_ceres_validation['sig_corr']\n",
    "            else:\n",
    "                cbar_dict = trend_cbar\n",
    "                to_hatch = (ds_ceres_validation['sig_diff_trends'] == 0).astype(int)  \n",
    "\n",
    "\n",
    "            ax = axes[row_ct, col_ct]\n",
    "            if is_grid:\n",
    "                to_plot = to_plot.where(analysis_mask)\n",
    "                im = to_plot.plot(ax=ax, norm=cbar_dict['norm'], cmap=cbar_dict['cmap'], add_colorbar=False)\n",
    "\n",
    "\n",
    "            else:\n",
    "                im = ax.scatter(this_lons, this_lats, c=to_plot, s=20, edgecolor='none', zorder=3,\n",
    "                                norm=cbar_dict['norm'], cmap=cbar_dict['cmap'], transform=ccrs.PlateCarree())\n",
    "\n",
    "            if to_hatch is not None:\n",
    "                to_hatch = to_hatch.where(analysis_mask)\n",
    "                contour = to_hatch.plot.contourf(ax=ax, levels=[-0.5, 0.5], hatches=['....', None], \n",
    "                                                 colors='none', add_colorbar=False, zorder=4)\n",
    "\n",
    "            ax.set_extent([-180, 180, -60, 80])\n",
    "\n",
    "            # Add map features\n",
    "            ax.add_feature(cfeature.BORDERS, linewidth=0.5, zorder=2)\n",
    "            ax.gridlines(draw_labels=False, linewidth=0.5, color='gray', alpha=0.5)\n",
    "            ax.coastlines(zorder=3, lw=1)\n",
    "            ax.add_feature(cfeature.LAND, color='darkgray', zorder=0)\n",
    "            ax.add_feature(cfeature.OCEAN, color='lightgray', zorder=1)\n",
    "            ax.set_title('')\n",
    "\n",
    "\n",
    "            ax.set_title('(%s) %s' % (letters[text_ct], fig_names[text_ct]))\n",
    "\n",
    "            text_ct += 1\n",
    "\n",
    "    # Get colorbar positions\n",
    "    pos1 = (fig.axes[1]).get_position()\n",
    "    cbar1_bottom = pos1.y0\n",
    "    cbar1_height = pos1.y1 - pos1.y0\n",
    "\n",
    "    pos2 = (fig.axes[3]).get_position()\n",
    "    pos3 = (fig.axes[5]).get_position()\n",
    "    cbar2_bottom = pos3.y0\n",
    "    cbar2_height = pos2.y1 - pos3.y0\n",
    "\n",
    "    # Add colorbars         \n",
    "    from matplotlib.cm import ScalarMappable\n",
    "\n",
    "    # Colorbar 1: top row\n",
    "    sm_top = ScalarMappable(cmap=pos_corr_cbar['cmap'], norm=pos_corr_cbar['norm'])\n",
    "    sm_top.set_array([])\n",
    "\n",
    "    cax_top = fig.add_axes([0.92, cbar1_bottom, 0.015, cbar1_height])  # [left, bottom, width, height]\n",
    "    cbar_top = fig.colorbar(sm_top, cax=cax_top, orientation='vertical', extend='min')\n",
    "    cbar_top.set_label('Correlation', fontsize=12)\n",
    "\n",
    "    # Colorbar 2: bottom two rows\n",
    "    sm_bot = ScalarMappable(cmap=trend_cbar['cmap'], norm=trend_cbar['norm'])\n",
    "    sm_bot.set_array([])\n",
    "\n",
    "    cax_bot = fig.add_axes([0.92, cbar2_bottom, 0.015, cbar2_height]) \n",
    "    cbar_bot = fig.colorbar(sm_bot, cax=cax_bot, orientation='vertical', extend='both')\n",
    "    cbar_bot.set_label(trend_str, fontsize=12)\n",
    "    \n",
    "    plt.savefig(figname, dpi=200, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "196caaa7",
   "metadata": {},
   "source": [
    "# Figure 2: ERA5 trends, causes, and case studies\n",
    "(a) ERA5 trend with hatching\n",
    "\n",
    "(b) AOD trends\n",
    "\n",
    "(c) Total cloud trends\n",
    "\n",
    "(d)-(h) Case studies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb92fb03",
   "metadata": {},
   "outputs": [],
   "source": [
    "era5_idx = np.where(['ERA5_%s' % (this_var) == n for n in name_list])[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74088f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load validation mask for ERA5\n",
    "ceres_savename = '%s/validation_metrics_%s.nc' % (procdir, 'ERA5')\n",
    "ds_validation = xr.open_dataset(ceres_savename)\n",
    "quality_mask = ds_validation['quality_mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "720385b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trends in AOD\n",
    "da_aod_ann = (my_xutils.compute_annual_mean_of_full_years(da_aod)).sel(year=slice('%04i' % years_to_use[0],\n",
    "                                                                                 '%04i' % years_to_use[1]))\n",
    "beta_aod = my_xutils.compute_linear_trend_per_year(da_aod_ann)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c3605a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trends in total cloud\n",
    "cloud_file = '/home/data/ERA5/month/total_cloud_cover/total_cloud_cover.nc'\n",
    "da_tc = xr.open_dataarray(cloud_file)\n",
    "da_tc = da_tc.rename({'latitude': 'lat', 'longitude': 'lon', 'valid_time': 'time'})\n",
    "da_tc = ((da_tc.sortby('lat'))).interp(lat=shared_lats, lon=shared_lons).load()\n",
    "da_tc_ann = (my_xutils.compute_annual_mean_of_full_years(da_tc)).sel(year=slice('%04i' % years_to_use[0],\n",
    "                                                                               '%04i' % years_to_use[1]))\n",
    "beta_tc = my_xutils.compute_linear_trend_per_year(da_tc_ann)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dade15a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 8))\n",
    "nrows = 6\n",
    "ncols = 2\n",
    "\n",
    "locs_case_studies = {'central/west US': [[30, 45], [235, 265]],\n",
    "                     'central South America': [[-35, 0], [295, 320]],\n",
    "                     'Europe': [[42, 57], [0, 32]],\n",
    "                     'southeastern China': [[21, 38], [100, 120]],\n",
    "                     'India': [[7, 29], [68, 90]]}\n",
    "\n",
    "cbar_names = (trend_str, \n",
    "              'AOD trend x 1000 ([]/year)',\n",
    "              'Total cloud trend (%/year)')\n",
    "\n",
    "# lefthand side: maps\n",
    "# righthand side: time series\n",
    "gs = gridspec.GridSpec(nrows=nrows, ncols=ncols, figure=fig,\n",
    "                       width_ratios=[1.5, 1], \n",
    "                       wspace=0.1, hspace=0.8)\n",
    "proj = ccrs.PlateCarree()\n",
    "\n",
    "# lefthand side\n",
    "for ct in range(3):\n",
    "    ax = fig.add_subplot(gs[(ct * 2):(ct * 2 + 2), 0], projection=proj)\n",
    "    # cbar_ax = fig.add_subplot(gs[(ct * 3 + 2), 0])\n",
    "    \n",
    "    if ct == 0:  # ERA5 trends\n",
    "        to_plot = trend_maps[era5_idx].where(analysis_mask).where(quality_mask)\n",
    "        to_hatch = quality_mask\n",
    "        cbar_dict = trend_cbar\n",
    "        extend = 'both'\n",
    "        \n",
    "    elif ct == 1:  # AOD trends\n",
    "        \n",
    "        to_plot = 1000 * beta_aod.where(analysis_mask)  # just to make units on colorbar nicer\n",
    "        levels = np.arange(-4, 4.1, 0.5)\n",
    "        norm = mcolors.BoundaryNorm(levels, ncolors=256)\n",
    "        cbar_dict = {'levels': levels, 'norm': norm, 'cmap': 'Spectral_r'}\n",
    "        extend = 'both'\n",
    "        to_hatch = None\n",
    "        \n",
    "    else:  # Cloud trend\n",
    "        to_plot = 100 * beta_tc  # 100 to switch from fraction to percent\n",
    "        to_plot = to_plot.where(quality_mask).where(analysis_mask)\n",
    "        to_hatch = quality_mask\n",
    "        levels = np.arange(-.15, .16, 0.03)\n",
    "        norm = mcolors.BoundaryNorm(levels, ncolors=256)\n",
    "        cbar_dict = {'levels': levels, 'norm': norm, 'cmap': 'RdBu'}\n",
    "        extend = 'both'\n",
    "\n",
    "    # Map look\n",
    "    ax.set_extent([-180, 180, -60, 80])\n",
    "    ax.coastlines(zorder=3, lw=1)\n",
    "    ax.add_feature(cfeature.BORDERS, linewidth=0.5, zorder=2)\n",
    "    ax.add_feature(cfeature.LAND, color='darkgray', zorder=0)\n",
    "    ax.add_feature(cfeature.OCEAN, color='lightgray', zorder=1)\n",
    "    ax.gridlines(draw_labels=False, linewidth=0.5, color='gray', alpha=0.5)\n",
    "    \n",
    "    # Plot without auto colorbar\n",
    "    im = to_plot.plot(ax=ax, norm=cbar_dict['norm'], cmap=cbar_dict['cmap'],\n",
    "                      add_colorbar=False)\n",
    "\n",
    "    # --- Horizontal colorbar under the map, same width ---\n",
    "    divider = make_axes_locatable(ax)\n",
    "    cax = divider.append_axes('bottom', size='6%', pad=0.2, axes_class=plt.Axes)\n",
    "    cb = fig.colorbar(im, cax=cax, orientation='horizontal', extend=extend)\n",
    "    cb.set_label(cbar_names[ct], labelpad=0)\n",
    "    cb.ax.tick_params(length=3)\n",
    "    \n",
    "\n",
    "    # Optional hatching\n",
    "    if to_hatch is not None:\n",
    "        to_hatch.plot.contourf(ax=ax, levels=[-0.5, 0.5],\n",
    "                               hatches=['....', None], colors='none',\n",
    "                               add_colorbar=False)\n",
    "    ax.set_title('')\n",
    "    ax.text(0.02, 0.1, '(%s)' % letters[ct], color='k', fontsize=12, \n",
    "            ha='left', transform=ax.transAxes)\n",
    "    \n",
    "\n",
    "    for name, ((lat_min, lat_max), (lon_min, lon_max)) in locs_case_studies.items():\n",
    "        width = lon_max - lon_min\n",
    "        height = lat_max - lat_min\n",
    "        rect = mpatches.Rectangle(\n",
    "            (lon_min, lat_min), width, height,\n",
    "            linewidth=1, edgecolor='tab:blue', facecolor='none',\n",
    "            transform=ccrs.PlateCarree(), zorder=5\n",
    "        )\n",
    "        ax.add_patch(rect)\n",
    "\n",
    "\n",
    "# righthand side\n",
    "for ct_loc, this_loc in enumerate(locs_case_studies.keys()):\n",
    "    these_coords = locs_case_studies[this_loc]\n",
    "\n",
    "    vars_to_plot = 'SW', 'total cloud', 'AOD', 'ISCCP'\n",
    "\n",
    "    all_ts = []\n",
    "\n",
    "    for v in vars_to_plot:\n",
    "        if v == 'SW':\n",
    "            this_da = all_sw[era5_idx]\n",
    "        elif v == 'total cloud':\n",
    "            this_da = ds_clouds_era5_coarse['total_cloud_cover'] * 100  # cloud percentage\n",
    "        elif v == 'AOD':\n",
    "            this_da = 1000 * da_aod  # nicer units\n",
    "        elif v == 'ISCCP':\n",
    "            this_da = (da_cldamt_isccp.sum('cloud_irtype'))\n",
    "            \n",
    "        this_da = this_da.sel(time=slice('%04i' % years_to_use[0], '%04i' % years_to_use[1]))\n",
    "        this_ts = this_da.sel(lat=slice(these_coords[0][0], these_coords[0][1]),\n",
    "                              lon=slice(these_coords[1][0], these_coords[1][1]))\n",
    "        this_ts = my_xutils.compute_annual_mean_of_full_years(this_ts.where(quality_mask==1))\n",
    "        this_ts = my_xutils.area_weighted_average(this_ts)\n",
    "        all_ts.append(this_ts.load())\n",
    "\n",
    "    ax = fig.add_subplot(gs[ct_loc, 1])\n",
    "\n",
    "    # Main axis\n",
    "    (all_ts[0] - all_ts[0].mean()).plot(label='ERA5 SW$_\\downarrow$', color='k')\n",
    "\n",
    "    # First twin axis (blue + purple)\n",
    "    ax2 = ax.twinx()\n",
    "    (all_ts[1] - all_ts[1].mean()).plot(ax=ax2, label='ERA5 total cloud (%)', color='tab:blue')\n",
    "    if this_loc != 'India':\n",
    "        (all_ts[3] - all_ts[3].mean()).plot(ax=ax2, label='ISCCP total cloud (%)', color='tab:blue', \n",
    "                                            ls='--')\n",
    "    else:\n",
    "        sub_plot = all_ts[3].sel(year=slice('1999', None))\n",
    "        (sub_plot - sub_plot.mean()).plot(ax=ax2, label='ISCCP total cloud (%)', color='tab:blue', \n",
    "                                            ls='--')\n",
    "    ax2.invert_yaxis()\n",
    "    ax2.spines['right'].set_position(('axes', 1))  # offset to the right\n",
    "    ax2.tick_params(axis='y', colors='tab:blue')\n",
    "    ax2.spines['right'].set_color('tab:blue')\n",
    "    ax2.yaxis.label.set_color('tab:blue')\n",
    "\n",
    "    # Second twin axis (orange)\n",
    "    ax3 = ax.twinx()\n",
    "    (all_ts[2] - all_ts[2].mean()).plot(ax=ax3, label='AOD x 1000', color='tab:orange')\n",
    "\n",
    "    ax3.spines['right'].set_position(('axes', 1.15))  # further offset\n",
    "    ax3.tick_params(axis='y', colors='tab:orange')\n",
    "    ax3.spines['right'].set_color('tab:orange')\n",
    "    ax3.yaxis.label.set_color('tab:orange')\n",
    "\n",
    "    if ct_loc == 0:\n",
    "        lines, labels = [], []\n",
    "        for a in [ax, ax2, ax3]:\n",
    "            lns, lbls = a.get_legend_handles_labels()\n",
    "            lines.extend(lns)\n",
    "            labels.extend(lbls)\n",
    "\n",
    "        fig.legend(lines, labels, loc='lower center', ncol=2, bbox_to_anchor=(0.77, 0.1))\n",
    "    # --- Force all axes to align zero in the middle ---\n",
    "    for a in [ax, ax2, ax3]:\n",
    "        ymin, ymax = a.get_ylim()\n",
    "        m = max(abs(ymin), abs(ymax))   # symmetric around 0\n",
    "        a.set_ylim(-m, m)\n",
    "        a.set_title('')\n",
    "        a.set_xlabel('')\n",
    "        a.set_xticks([])\n",
    "        a.set_xlim(years_to_use[0] - 2, years_to_use[1] + 2)\n",
    "    ax2.invert_yaxis()\n",
    "    ax3.invert_yaxis()\n",
    "    ax.set_title('(%s) %s' % (letters[ct + ct_loc + 1], this_loc))\n",
    "ax.set_xticks(np.arange(years_to_use[0], years_to_use[1] + 5, 10))\n",
    "\n",
    "plt.savefig('%s/fig02.png' % figdir, dpi=200, bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d9f3666",
   "metadata": {},
   "source": [
    "# Figure 3\n",
    "\n",
    "4 panels:\n",
    "- CMIP6 MMM \n",
    "- ranks of ERA5\n",
    "- histograms of ranks\n",
    "- similarity histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dade0d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CMIP6 trends and calculate rank of obs\n",
    "# Calculated in a different notebook using Pangea data\n",
    "\n",
    "cmip6_fname = 'CMIP6_rsds_historical-ssp370_%04i-%04i.nc' % (years_to_use[0], years_to_use[1])\n",
    "cmip6_fname_ssp245 = 'CMIP6_rsds_historical-ssp245_1980-2024.nc'\n",
    "\n",
    "cmip6_trends = xr.open_dataarray('%s/%s' %  (procdir, cmip6_fname))\n",
    "cmip6_trends_ssp245 = xr.open_dataarray('%s/%s' %  (procdir, cmip6_fname_ssp245))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab3cefc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'ssp2' in figdir:\n",
    "    # Subset to shared models for consistency with main results\n",
    "    shared_models = np.isin(cmip6_trends_ssp245.base_model, cmip6_trends.base_model)\n",
    "    cmip6_trends = cmip6_trends_ssp245.isel(model=shared_models)\n",
    "    cmip6_trends = cmip6_trends.drop('base_model')\n",
    "    model_names = ([m.split('_')[0] for m in cmip6_trends.model.values])\n",
    "    cmip6_trends = cmip6_trends.assign_coords(base_model=('model', model_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ae41495",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_means = cmip6_trends.groupby('base_model').mean(dim='model')\n",
    "MMM = model_means.mean(dim='base_model')\n",
    "if 'ssp2' in figdir:\n",
    "    ens_names = ['CMIP6']   \n",
    "else:\n",
    "    ens_names = ['CESM2', 'CMIP6']\n",
    "\n",
    "obs_trend = trend_maps[era5_idx]\n",
    "obs_trend_masked = obs_trend.where(quality_mask).where(analysis_mask)\n",
    "\n",
    "all_ranks = []\n",
    "\n",
    "# Collect trends across ensembles\n",
    "# And also calculate rank within each ensemble\n",
    "all_members = []\n",
    "for ens_name in ens_names:\n",
    "    print(ens_name)\n",
    "    if ens_name == 'CMIP6':\n",
    "        ens_trends = cmip6_trends\n",
    "        ens_dim_name = 'model'\n",
    "    else:\n",
    "        this_str = '%s_%s' % (ens_name, this_var)\n",
    "        this_idx = np.where([this_str == n for n in name_list])[0][0]\n",
    "        ens_trends = trend_maps[this_idx]\n",
    "        ens_dim_name = 'member'\n",
    "        \n",
    "    rank_obs_with_ensemble = xr.apply_ufunc(\n",
    "        my_utils.rank_func,\n",
    "        ens_trends,\n",
    "        obs_trend,\n",
    "        input_core_dims=[[ens_dim_name], []],\n",
    "        output_core_dims=[[]],\n",
    "        vectorize=True,\n",
    "        dask='parallelized',\n",
    "        output_dtypes=[float]\n",
    "    )\n",
    "    \n",
    "    all_ranks.append(rank_obs_with_ensemble.rename('ERA5_in_%s' % ens_name))\n",
    "    if ens_name == 'CMIP6':\n",
    "        ens_trends = ens_trends.rename({'model': 'member'}).drop('base_model')\n",
    "    all_members.append(ens_trends)\n",
    "all_members = xr.concat(all_members, dim='member')\n",
    "\n",
    "rank_obs_with_ensemble = xr.apply_ufunc(\n",
    "    my_utils.rank_func,\n",
    "    all_members,\n",
    "    obs_trend,\n",
    "    input_core_dims=[['member'], []],\n",
    "    output_core_dims=[[]],\n",
    "    vectorize=True,\n",
    "    dask='parallelized',\n",
    "    output_dtypes=[float]\n",
    ")\n",
    "all_ranks.append(rank_obs_with_ensemble.rename('ERA5_in_all_ensembles'))\n",
    "all_ranks = xr.merge(all_ranks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21e277d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For null hypothesis later, also rank each member of the super-ensemble against the obs\n",
    "savename = '%s/null_ranks_%s_%04i-%04i.nc' % (procdir, '_'.join(ens_names), years_to_use[0], years_to_use[1])\n",
    "\n",
    "if os.path.isfile(savename):  # slow to run\n",
    "    null_ranks = xr.open_dataarray(savename)\n",
    "else:\n",
    "    null_ranks = []\n",
    "    for this_member in all_members.member.values:\n",
    "        sub_ens = all_members.drop_sel(member=this_member)\n",
    "        sub_ens = xr.concat((sub_ens, obs_trend.expand_dims(member=['obs'])), dim='member')\n",
    "        this_run = all_members.sel(member=this_member)\n",
    "        rank_runs_with_ensemble = xr.apply_ufunc(\n",
    "            my_utils.rank_func,\n",
    "            sub_ens,\n",
    "            this_run,\n",
    "            input_core_dims=[['member'], []],\n",
    "            output_core_dims=[[]],\n",
    "            vectorize=True,\n",
    "            dask='parallelized',\n",
    "            output_dtypes=[float]\n",
    "        )\n",
    "        null_ranks.append(rank_runs_with_ensemble)\n",
    "    null_ranks = xr.concat(null_ranks, dim='member')\n",
    "    null_ranks['member'] = all_members['member']\n",
    "    null_ranks.to_netcdf(savename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c298ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_names = 'CMIP6 MMM', 'Rank of ERA5 within %s' % '+'.join(ens_names)\n",
    "cbar_names = trend_str, 'Rank'\n",
    "\n",
    "nrows = 2\n",
    "ncols = 2\n",
    "fig = plt.figure(figsize=(6 * ncols, 4 * nrows))\n",
    "proj = ccrs.PlateCarree()\n",
    "gs = gridspec.GridSpec(\n",
    "    nrows=nrows, ncols=ncols, figure=fig,\n",
    "    height_ratios=[1, 0.6],  # a bit more room for maps + colorbars\n",
    "    hspace=0, wspace=0.18)\n",
    "\n",
    "for ct in range(4):\n",
    "    if ct < 2:  # first row, maps\n",
    "        ax = fig.add_subplot(gs[0, ct], projection=proj)\n",
    "    else:  # second row, histograms\n",
    "        ax = fig.add_subplot(gs[1, ct % 2])\n",
    "        \n",
    "    if ct < 2:\n",
    "        if ct == 0:  # CMIP trends\n",
    "            to_plot = MMM.where(analysis_mask)\n",
    "            cbar_dict = trend_cbar\n",
    "            to_hatch = None\n",
    "            extend = 'both'\n",
    "        else: \n",
    "            to_plot = all_ranks['ERA5_in_all_ensembles'].where(analysis_mask)\n",
    "            to_plot = to_plot.where(quality_mask)\n",
    "            rank_min, rank_max = int(to_plot.min()), int(to_plot.max())\n",
    "\n",
    "            ranks_edge = int(np.ceil(0.025 * (rank_max)))\n",
    "            levels = np.hstack((np.arange(rank_min - 0.5, rank_min + ranks_edge),\n",
    "                                np.arange(rank_max + 0.5 - ranks_edge, rank_max + 1.5)))\n",
    "            norm = mcolors.BoundaryNorm(levels, ncolors=256)\n",
    "            cbar_dict = {'levels': levels, 'norm': norm, 'cmap': 'RdBu_r'}\n",
    "            extend = 'neither'\n",
    "            to_hatch = quality_mask\n",
    "\n",
    "\n",
    "        ax.set_extent([-180, 180, -60, 80])\n",
    "        ax.coastlines(zorder=3, lw=1)\n",
    "        ax.add_feature(cfeature.BORDERS, linewidth=0.5, zorder=2)\n",
    "        ax.add_feature(cfeature.LAND, color='darkgray', zorder=0)\n",
    "        ax.add_feature(cfeature.OCEAN, color='lightgray', zorder=1)\n",
    "        ax.gridlines(draw_labels=False, linewidth=0.5, color='gray', alpha=0.5)\n",
    "\n",
    "\n",
    "        im = to_plot.plot(ax=ax, norm=cbar_dict['norm'], cmap=cbar_dict['cmap'],\n",
    "                          add_colorbar=False)\n",
    "\n",
    "        divider = make_axes_locatable(ax)\n",
    "        cax = divider.append_axes('bottom', size='6%', pad=0.15, axes_class=plt.Axes)\n",
    "        cb = fig.colorbar(im, cax=cax, orientation='horizontal', extend=extend)\n",
    "        cb.set_label(cbar_names[ct], labelpad=0)\n",
    "        cb.ax.tick_params(length=3)\n",
    "        if ct == 1:\n",
    "            mid_bins = np.hstack(((np.ceil(levels)[:ranks_edge]).astype(int), \n",
    "                                  (np.ceil(levels)[-(ranks_edge + 1):-1]).astype(int)))\n",
    "            cb.set_ticks(mid_bins)\n",
    "\n",
    "        # Optional hatching\n",
    "        if to_hatch is not None:\n",
    "            to_hatch.plot.contourf(ax=ax, levels=[-0.5, 0.5],\n",
    "                                   hatches=['....', None], colors='none',\n",
    "                                   add_colorbar=False)\n",
    "        ax.set_title('(%s) %s' % (letters[ct], fig_names[ct]))\n",
    "\n",
    "\n",
    "    if ct == 2:  # rank histogram\n",
    "        \n",
    "        tail_width = 6\n",
    "        \n",
    "        # Calculate histogram of ranks in non-masked regions\n",
    "        hist_vals_era5, bin_middle = my_utils.get_rank_hist(all_ranks['ERA5_in_all_ensembles'], \n",
    "                                                            quality_mask, \n",
    "                                                            rank_max)\n",
    "        ax.set_xlabel('Rank')\n",
    "        ax.set_ylabel('Area fraction')\n",
    "        \n",
    "        # plot null\n",
    "        keep_max_null = 0\n",
    "        max_idx = np.nan\n",
    "        \n",
    "        all_hist_vals_null = np.empty((len(null_ranks.member), len(bin_middle)))\n",
    "        for kk in range(len(null_ranks.member)):\n",
    "            hist_vals_null, _ = my_utils.get_rank_hist(null_ranks.isel(member=kk), \n",
    "                                                       quality_mask, \n",
    "                                                       rank_max)\n",
    "            if hist_vals_null[-1] > keep_max_null:\n",
    "                keep_max_null = hist_vals_null[-1]\n",
    "                max_idx = kk\n",
    "            all_hist_vals_null[kk, :] = hist_vals_null\n",
    "\n",
    "            ax.plot(bin_middle, hist_vals_null, color='gray', lw=0.5, alpha=0.5)\n",
    "        ax.plot(bin_middle, hist_vals_null, color='gray', lw=0.5, alpha=0.5, label='Null density samples')\n",
    "        ax.plot(bin_middle[-1], keep_max_null, marker='*', color='k', markersize=5, \n",
    "               label='Maximum null density')\n",
    "        ax.plot(bin_middle, all_hist_vals_null[max_idx, :], color='k')\n",
    "        ax.plot(bin_middle, hist_vals_era5, color='tab:red', label='ERA5 density')\n",
    "        ax.legend(loc='upper left')\n",
    "        ax.set_title('(%s) Rank histogram' % (letters[ct]))\n",
    "        \n",
    "        # Inset axis\n",
    "        inset_ax = ax.inset_axes([0.63, 0.55, 0.3, 0.35])  \n",
    "        inset_ax.hist(all_hist_vals_null[:, bin_middle > (rank_max - tail_width)].sum(axis=1), color='gray')\n",
    "        inset_ax.axvline(hist_vals_era5[bin_middle > (rank_max - tail_width)].sum(), color='tab:red')\n",
    "\n",
    "        inset_ax.set_xlabel('Area fraction in\\nmax %i ranks' % tail_width, fontsize=8)\n",
    "        inset_ax.set_ylabel('Count', fontsize=8)\n",
    "        inset_ax.tick_params(labelsize=8)\n",
    "\n",
    "        for spine in inset_ax.spines.values():\n",
    "            spine.set_linewidth(0.8)\n",
    "        \n",
    "    elif ct == 3:  # correlation histograms plot\n",
    "        nmembers = len(all_members.member)\n",
    "        trends_masked = all_members.where(quality_mask).where(analysis_mask)\n",
    "        \n",
    "        savename = '%s/pattern_corr_ERA5_%s_%04i-%04i.npz' % (procdir, '_'.join(ens_names), \n",
    "                                                              years_to_use[0], years_to_use[1])\n",
    "        \n",
    "        if os.path.isfile(savename):\n",
    "            rho_load = np.load(savename)\n",
    "            rho_member_trends = rho_load['rho_member_trends']\n",
    "            rho_member_with_era5 = rho_load['rho_member_with_era5']\n",
    "        else:\n",
    "    \n",
    "            rho_member_trends = np.nan * np.ones((nmembers, nmembers))\n",
    "            rho_member_with_era5 = np.nan * np.ones((nmembers, ))\n",
    "            for ct1, m1 in enumerate(trends_masked.member):\n",
    "\n",
    "                rho_member_with_era5[ct1] = my_xutils.xr_weighted_corr(trends_masked.sel(member=m1),\n",
    "                                                                       obs_trend_masked)\n",
    "\n",
    "                for ct2, m2 in enumerate(trends_masked.member):\n",
    "                    if ct2 <= ct1:\n",
    "                        continue\n",
    "                    this_rho = my_xutils.xr_weighted_corr(trends_masked.sel(member=m1), \n",
    "                                                          trends_masked.sel(member=m2))\n",
    "                    rho_member_trends[ct1, ct2] = this_rho\n",
    "\n",
    "            np.savez(savename, rho_member_trends=rho_member_trends, rho_member_with_era5=rho_member_with_era5)\n",
    "        \n",
    "        print('Mean ERA5-model corr: %0.2f' % np.mean(rho_member_with_era5))\n",
    "        print('Mean model-model corr: %0.2f' % np.nanmean(rho_member_trends.flatten()))\n",
    "        bins = np.arange(0.2, 1, 0.02)\n",
    "        ax.hist(rho_member_trends.flatten()[~np.isnan(rho_member_trends.flatten())], density=True, \n",
    "                label='Within %s' % '+'.join(ens_names), color='k', bins=bins)\n",
    "        ax.hist(rho_member_with_era5, density=True, label='ERA5-%s' % '+'.join(ens_names), \n",
    "                alpha=0.8, color='tab:blue', bins=bins)\n",
    "        ax.legend()\n",
    "        ax.set_xlabel('Correlation')\n",
    "        ax.set_title('(%s) Spatial correlation histograms' % (letters[ct]))\n",
    "        \n",
    "plt.savefig('%s/fig03.png' % figdir, dpi=200, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44c31a94",
   "metadata": {},
   "source": [
    "# Supplemental figures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0c74691",
   "metadata": {},
   "source": [
    "## Trends in all products without ocean masked \n",
    "ERA5, JRA-3Q, MERRA2, CLARA, GEWEX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe475f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot\n",
    "nrows, ncols = 3, 2\n",
    "fig, axes = plt.subplots(\n",
    "    nrows=nrows, ncols=ncols,\n",
    "    figsize=(6 * ncols, 3 * nrows),\n",
    "    subplot_kw={'projection': ccrs.PlateCarree()},\n",
    "    gridspec_kw={'hspace': 0.4, 'wspace': 0.15}\n",
    ")\n",
    "\n",
    "for ct, rname in enumerate(reanalysis_names):\n",
    "    ax = axes.flatten()[ct]\n",
    "    this_idx = np.where(['%s_%s' % (rname, this_var) == n for n in name_list])[0][0]\n",
    "    to_plot = trend_maps[this_idx]\n",
    "    this_start_year = start_year[this_idx]\n",
    "    this_end_year = end_year[this_idx]\n",
    "    cbar_dict = trend_cbar\n",
    "    extend = 'both'\n",
    "    \n",
    "    # Map look\n",
    "    ax.set_extent([-180, 180, -60, 80])\n",
    "    ax.coastlines(zorder=3, lw=1)\n",
    "    ax.add_feature(cfeature.BORDERS, linewidth=0.5, zorder=2)\n",
    "    ax.add_feature(cfeature.LAND, color='darkgray', zorder=0)\n",
    "    ax.add_feature(cfeature.OCEAN, color='lightgray', zorder=1)\n",
    "    ax.gridlines(draw_labels=False, linewidth=0.5, color='gray', alpha=0.5)\n",
    "    \n",
    "\n",
    "    # Plot without auto colorbar\n",
    "    im = to_plot.plot(ax=ax, norm=cbar_dict['norm'], cmap=cbar_dict['cmap'],\n",
    "                      add_colorbar=False)\n",
    "    # --- Horizontal colorbar under the map, same width ---\n",
    "    divider = make_axes_locatable(ax)\n",
    "    cax = divider.append_axes('bottom', size='6%', pad=0.15, axes_class=plt.Axes)\n",
    "    cb = fig.colorbar(im, cax=cax, orientation='horizontal', extend=extend)\n",
    "    cb.set_label(trend_str, labelpad=0)\n",
    "    cb.ax.tick_params(length=3)\n",
    "\n",
    "    ax.set_title('(%s) %s (%i-%i)' % (letters[ct], rname, this_start_year, this_end_year))\n",
    "fig.delaxes(axes.flatten()[-1])\n",
    "\n",
    "plt.savefig('%s/SUPP_all_trends_with_ocean.png' % figdir, dpi=200, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83e78a16",
   "metadata": {},
   "source": [
    "## Histograms for GEBA correlations across different products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa5fc73",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_geba_interannual_long = np.load('%s/geba_reanalysis_with_RS_corr_full_record_20yearsmin.npz' % procdir)\n",
    "ds_geba_interannual_preceres = np.load('%s/geba_reanalysis_with_RS_corr_pre_ceres_10yearsmin.npz' % procdir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3926b0ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = np.arange(-1, 1.01, 0.1)\n",
    "fig, ax = plt.subplots(figsize=(7, 3), ncols=2, nrows=1, sharey=True)\n",
    "fig.tight_layout()\n",
    "colors = ['k', 'tab:blue', 'tab:green', 'tab:purple', 'tab:orange']\n",
    "xmin, xmax = -1.0, 1.0\n",
    "xx = np.linspace(xmin, xmax, 512)\n",
    "\n",
    "for hist_ct in range(2):\n",
    "    this_ax = ax[hist_ct]\n",
    "    if hist_ct == 0:\n",
    "        to_plot = ds_geba_interannual_long\n",
    "        title = 'GEBA correlation\\n20y+ records'\n",
    "    else:\n",
    "        to_plot = ds_geba_interannual_preceres\n",
    "        title = 'GEBA correlation pre-CERES\\n10y+ records'\n",
    "    alpha = 1\n",
    "    for o_ct, rname in enumerate(reanalysis_names):\n",
    "        # pull values, drop NaNs\n",
    "        x = np.asarray(to_plot['rho_geba_reanalysis'][o_ct, :]).ravel()\n",
    "        x = x[np.isfinite(x)]\n",
    "        if x.size == 0:\n",
    "            continue\n",
    "\n",
    "        kde = gaussian_kde(x, bw_method='scott')\n",
    "        yy = kde(xx)\n",
    "\n",
    "        # plot smooth density and its median\n",
    "        this_ax.plot(xx, yy, lw=2, alpha=alpha, color=colors[o_ct], label=rname)\n",
    "        med = np.median(x)\n",
    "        print('%s, %0.2f' % (rname, med))\n",
    "        this_ax.axvline(med, lw=2, alpha=alpha, color=colors[o_ct])\n",
    "\n",
    "        # optional light fill under curve\n",
    "        this_ax.fill_between(xx, 0, yy, alpha=0.1, color=colors[o_ct])\n",
    "\n",
    "        alpha -= 0.1\n",
    "\n",
    "    this_ax.set_xlim(xmin, xmax)\n",
    "    this_ax.set_ylim(bottom=0)\n",
    "    this_ax.set_title(title)\n",
    "    this_ax.set_xlabel('Correlation')\n",
    "    if hist_ct == 0:\n",
    "        this_ax.set_ylabel('Density')\n",
    "\n",
    "# one legend is enough\n",
    "ax[0].legend(frameon=False, ncol=1, title=None)\n",
    "plt.savefig('%s/SUPP_GEBA_corr_histograms.png' % figdir, dpi=200, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "558cd86e",
   "metadata": {},
   "source": [
    "## ERA5 clearsky "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c4b68b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load ERA5 clearsky, and show trends\n",
    "savename = '%s/ERA5_clearsky_rg.nc' % procdir\n",
    "if os.path.isfile(savename):\n",
    "    da_era5_clearsky = xr.load_dataarray(savename)\n",
    "else:\n",
    "    csvar = 'surface_solar_radiation_downward_clear_sky'\n",
    "    da_era5_clearsky = xr.open_dataarray('%s/%s/%s.nc' % (data_dirs['ERA5'], csvar, csvar))\n",
    "    # Same processing as SW_down\n",
    "    da_era5_clearsky = da_era5_clearsky.rename({'valid_time': 'time'})\n",
    "    adjusted_time = da_era5_clearsky.time.dt.floor('D')\n",
    "    da_era5_clearsky['time'] = adjusted_time\n",
    "    # Heating is in J/m2\n",
    "    da_era5_clearsky /= sec_per_day\n",
    "    da_era5_clearsky = da_era5_clearsky.rename({'latitude': 'lat', 'longitude': 'lon'})\n",
    "    da_era5_clearsky = da_era5_clearsky.sortby('lat')\n",
    "    \n",
    "    da_interp = my_utils.regrid_to_shared_grid(da_era5_clearsky, shared_lats, shared_lons)\n",
    "    da_era5_clearsky = da_interp.sel(time=slice('%04i' % years_to_use[0], '%04i' % years_to_use[1]))\n",
    "    del da_interp, da_pad\n",
    "    da_era5_clearsky.to_netcdf(savename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec4c39ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "beta_cs = my_xutils.compute_linear_trend_per_year(\n",
    "    my_xutils.compute_annual_mean_of_full_years(da_era5_clearsky))\n",
    "name = 'ERA5 SW$_\\downarrow$ clearsky trend (W/m$^2$/yr)'\n",
    "plot_global_discrete(beta_cs.sel(lat=slice(-60, 80)).rename(name), levels=trend_cbar['levels'] / 2)\n",
    "\n",
    "corr_with_AOD = my_xutils.xr_weighted_corr(beta_cs.where(analysis_mask), beta_aod.where(analysis_mask))\n",
    "print('Correlation with MERRA2 AOD trends (analysis region only): %0.2f' % corr_with_AOD)\n",
    "\n",
    "plt.savefig('%s/SUPP_ERA5_clearsky_trends.png' % figdir, dpi=200, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5220b60",
   "metadata": {},
   "source": [
    "## Correlation of SW at the surface and cloud fraction at different levels, ERA5 and CERES/ISCCP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "818958aa",
   "metadata": {},
   "source": [
    "### CERES / ISCCP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db17c4c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add total variable\n",
    "da_cldamt_isccp_incl_total = xr.concat((da_cldamt_isccp, \n",
    "                                        da_cldamt_isccp.sum('cloud_irtype').rename('total')), \n",
    "                                       dim='cloud_irtype')\n",
    "isccp_cloud_names = 'low', 'middle', 'high', 'total'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adb15c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict SW at surface using co-albedo estimate from clouds (scaling is irrelevant)\n",
    "cloud_pred = da_ceres_toa * (1 - da_cldamt_isccp_incl_total / 100)\n",
    "# Do comparison at the annual mean level\n",
    "cloud_pred_ann = my_xutils.compute_annual_mean_of_full_years(cloud_pred)\n",
    "\n",
    "ceres_idx = np.where(['%s_%s' % ('CERES', this_var) == n for n in name_list])[0][0]\n",
    "rho_clouds = xr.corr(my_xutils.compute_annual_mean_of_full_years(all_sw[ceres_idx]),\n",
    "                     cloud_pred_ann, dim='year')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc27e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make plot\n",
    "nrows = 2\n",
    "ncols = 2\n",
    "fig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=(8 * ncols, 3 * nrows),\n",
    "                         subplot_kw={'projection': ccrs.PlateCarree()})\n",
    "levels = np.arange(0, 1.1, 0.1)\n",
    "cmap = 'Reds'\n",
    "norm = BoundaryNorm(levels, ncolors=256)\n",
    "\n",
    "for ct, ax in enumerate(axes.flatten()):\n",
    "\n",
    "    im = rho_clouds.isel(cloud_irtype=ct).sel(lat=slice(-60, 80)).plot(ax=ax, \n",
    "                                                                       cmap=cmap,\n",
    "                                                                       norm=norm,\n",
    "                                                                       levels=levels,\n",
    "                                                                       add_colorbar=False)\n",
    "    ax.coastlines()\n",
    "    ax.add_feature(cfeature.BORDERS, linewidth=0.5)\n",
    "    ax.add_feature(cfeature.OCEAN, facecolor='lightgray', zorder=1)\n",
    "    ax.text(0.02, 0.1, '%s' % isccp_cloud_names[ct], color='k', fontsize=12, \n",
    "                               ha='left', transform=ax.transAxes)\n",
    "                                                                       \n",
    "# Shared colorbar\n",
    "cbar_ax = fig.add_axes([0.92, 0.15, 0.015, 0.7])\n",
    "cb = fig.colorbar(im, cax=cbar_ax, ticks=levels, extend='neither')\n",
    "cb.set_label('Correlation', fontsize=12)\n",
    "\n",
    "plt.savefig('%s/SUPP_CERES_ISCCP_cloud_SW_corr.png' % figdir, dpi=200, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bebfc161",
   "metadata": {},
   "source": [
    "### ERA5\n",
    "\n",
    "- Low cloud is a single level field calculated from cloud occurring on model levels with a pressure greater than 0.8 times the surface pressure.\n",
    "- Medium cloud is a single level field calculated from cloud occurring on model levels with a pressure between 0.45 and 0.8 times the surface pressure.\n",
    "- High cloud is a single level field calculated from cloud occurring on model levels with a pressure less than 0.45 times the surface pressure. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f396b22a",
   "metadata": {},
   "outputs": [],
   "source": [
    "era5_cloud_names = ['%s_cloud_cover' % c for c in list(('low', 'medium', 'high', 'total'))]\n",
    "savename = '%s/rho_clouds_era5.nc' % procdir\n",
    "if os.path.isfile(savename):\n",
    "    rho_clouds_era5 = xr.open_dataarray(savename)\n",
    "else:\n",
    "    ds_clouds = []\n",
    "    for var in era5_cloud_names:\n",
    "        this_dir = '%s/%s' % (data_dirs['ERA5'], var)\n",
    "        da = xr.open_dataarray('%s/%s.nc' % (this_dir, var))\n",
    "        da = da.rename({'valid_time': 'time'})\n",
    "        adjusted_time = da.time.dt.floor('D')\n",
    "        da['time'] = adjusted_time\n",
    "        da = da.rename({'latitude': 'lat', 'longitude': 'lon'})\n",
    "        da = da.sortby('lat').rename(var)\n",
    "        da = da.sel(time=slice('%04i' % years_to_use[0], \n",
    "                               '%04i' % years_to_use[1]))\n",
    "        da = my_utils.regrid_to_shared_grid(da, shared_lats, shared_lons)\n",
    "        ds_clouds.append(da.load())\n",
    "    ds_clouds = xr.merge(ds_clouds)\n",
    "\n",
    "    # Load TOA\n",
    "    toa_var = 'toa_incident_solar_radiation'\n",
    "    this_dir = '%s/%s' % (data_dirs['ERA5'], toa_var)\n",
    "    da_toa = xr.open_dataarray('%s/%s.nc' % (this_dir, toa_var))\n",
    "    da_toa = da_toa.rename({'valid_time': 'time'}).drop('expver')\n",
    "    adjusted_time = da_toa.time.dt.floor('D')\n",
    "    da_toa['time'] = adjusted_time\n",
    "    da_toa = da_toa.rename({'latitude': 'lat', 'longitude': 'lon'})\n",
    "    da_toa = da_toa.sortby('lat').rename(toa_var)\n",
    "    da_toa = my_utils.regrid_to_shared_grid(da_toa, shared_lats, shared_lons)\n",
    "    da_toa = da_toa.sel(time=slice('%04i' % years_to_use[0], '%04i' % years_to_use[1]))\n",
    "\n",
    "\n",
    "\n",
    "    pred_surf_sw = (da_toa * (1 - ds_clouds)).to_array(dim='cloud_type')\n",
    "\n",
    "    rho_clouds_era5 = xr.corr(my_xutils.compute_annual_mean_of_full_years(all_sw[era5_idx]),\n",
    "                              my_xutils.compute_annual_mean_of_full_years(pred_surf_sw), dim='year')\n",
    "\n",
    "    \n",
    "    rho_clouds_era5.to_netcdf(savename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c461f635",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make plot\n",
    "nrows = 2\n",
    "ncols = 2\n",
    "fig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=(8 * ncols, 3 * nrows),\n",
    "                         subplot_kw={'projection': ccrs.PlateCarree()})\n",
    "levels = np.arange(0, 1.1, 0.1)\n",
    "cmap = 'Reds'\n",
    "norm = BoundaryNorm(levels, ncolors=256)\n",
    "\n",
    "for ct, ax in enumerate(axes.flatten()):\n",
    "\n",
    "    im = rho_clouds_era5.isel(cloud_type=ct).sel(lat=slice(-60, 80)).plot(ax=ax, \n",
    "                                                                       cmap=cmap,\n",
    "                                                                       norm=norm,\n",
    "                                                                       levels=levels,\n",
    "                                                                       add_colorbar=False)\n",
    "    ax.coastlines()\n",
    "    ax.add_feature(cfeature.BORDERS, linewidth=0.5)\n",
    "    ax.add_feature(cfeature.OCEAN, facecolor='lightgray', zorder=1)\n",
    "    ax.text(0.02, 0.1, '%s' % era5_cloud_names[ct].split('_')[0], color='k', fontsize=12, \n",
    "                               ha='left', transform=ax.transAxes)\n",
    "    ax.set_title('')\n",
    "                                                                       \n",
    "# Shared colorbar\n",
    "cbar_ax = fig.add_axes([0.92, 0.15, 0.015, 0.7])\n",
    "cb = fig.colorbar(im, cax=cbar_ax, ticks=levels, extend='neither')\n",
    "cb.set_label('Correlation', fontsize=12)\n",
    "plt.savefig('%s/SUPP_ERA5_cloud_SW_corr.png' % figdir, dpi=200, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0519838",
   "metadata": {},
   "source": [
    "## Postage stamps for models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1622e6d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "cmip6_trends_ens_mean = cmip6_trends.groupby('base_model').mean()\n",
    "cmip6_trends_ens_count = cmip6_trends[:, 10, 10].groupby('base_model').count('model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e7add3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in cmip6_trends_ens_count.base_model:\n",
    "    print('%s, %i' % (model.values, cmip6_trends_ens_count.sel(base_model=model)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17ff2435",
   "metadata": {},
   "outputs": [],
   "source": [
    "nmodels = len(cmip6_trends_ens_mean.base_model)\n",
    "\n",
    "nrows = 12\n",
    "ncols = 3\n",
    "\n",
    "cmap = 'RdBu_r'\n",
    "\n",
    "fig, axes = plt.subplots(figsize=(15, 25), nrows=nrows, ncols=ncols, \n",
    "                         subplot_kw={'projection': ccrs.PlateCarree()})\n",
    "axes = axes.flatten()\n",
    "\n",
    "for m_ct, model_name in enumerate(cmip6_trends_ens_mean.base_model):\n",
    "    ax = axes[m_ct]\n",
    "    \n",
    "    if model_name == 'CESM2':  # use a bigger EM since we have it\n",
    "        cesm2_idx = np.where(['CESM2_%s' % this_var == n for n in name_list])[0][0]\n",
    "        idx = cmip6_trends['base_model'] == 'CESM2'\n",
    "        this_ens = xr.concat((cmip6_trends.sel(model=idx).drop('base_model'), \n",
    "                              trend_maps[cesm2_idx].rename({'member': 'model'})), dim='model')\n",
    "        to_plot = this_ens.mean('model')\n",
    "        this_count = len(this_ens['model'])\n",
    "    else:\n",
    "        to_plot = cmip6_trends_ens_mean.sel(base_model=model_name)\n",
    "        this_count = cmip6_trends_ens_count.sel(base_model=model_name)\n",
    "    im = (to_plot.sel(lat=slice(-60, 80))).plot(ax=ax, cmap=trend_cbar['cmap'], \n",
    "                                                norm=trend_cbar['norm'], \n",
    "                                                levels=trend_cbar['levels'],\n",
    "                                                      add_colorbar=False)\n",
    "          \n",
    "    ax.coastlines()\n",
    "    ax.add_feature(cfeature.BORDERS, linewidth=0.5)\n",
    "    ax.add_feature(cfeature.OCEAN, facecolor='lightgray', zorder=1)\n",
    "    \n",
    "    ax.set_title('%s (%i)' % (str(model_name.values), int(this_count)))\n",
    "\n",
    "# Shared colorbar\n",
    "cbar_ax = fig.add_axes([0.95, 0.3, 0.015, 0.4])\n",
    "cb = fig.colorbar(im, cax=cbar_ax, extend='both')\n",
    "cb.set_label(trend_str, fontsize=12)\n",
    "fig.delaxes(axes[-1])\n",
    "\n",
    "plt.savefig('%s/SUPP_CMIP_postage_stamps.png' % figdir, dpi=200, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fd438ff",
   "metadata": {},
   "source": [
    "## Role of internal variability: S2N for CESM2 and CanESM2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df71ab06",
   "metadata": {},
   "outputs": [],
   "source": [
    "cesm2_s2n = np.abs(trend_maps[cesm2_idx].mean('member')) / trend_maps[cesm2_idx].std('member')\n",
    "da_canesm = cmip6_trends.sel(model=cmip6_trends['base_model'] == 'CanESM5')\n",
    "canesm_s2n = np.abs(da_canesm.mean('model')) / da_canesm.std('model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dada130",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_names = 'CESM2 S2N', 'CanESM5 S2N'\n",
    "nrows, ncols = 2, 1\n",
    "fig, axes = plt.subplots(\n",
    "    nrows=nrows, ncols=ncols,\n",
    "    figsize=(7 * ncols, 3 * nrows),\n",
    "    subplot_kw={'projection': ccrs.PlateCarree()},\n",
    "    gridspec_kw={'hspace': 0.4, 'wspace': 0.15}\n",
    ")\n",
    "\n",
    "vmin, vmax, step = 0, 3, 0.2\n",
    "levels = np.arange(vmin, vmax + step, step)\n",
    "norm = mcolors.BoundaryNorm(levels, ncolors=256)\n",
    "cbar_dict = {'levels': levels, 'norm': norm, 'cmap': 'Purples'}\n",
    "extend = 'max'\n",
    "\n",
    "ims = []  # collect plotted images\n",
    "\n",
    "for ct, ax in enumerate(axes.flatten()):\n",
    "    if ct == 0:\n",
    "        to_plot = cesm2_s2n\n",
    "    else:\n",
    "        to_plot = canesm_s2n\n",
    "\n",
    "    to_plot = to_plot.where(analysis_mask)\n",
    "\n",
    "    # Map look\n",
    "    ax.set_extent([-180, 180, -60, 80])\n",
    "    ax.coastlines(zorder=3, lw=1)\n",
    "    ax.add_feature(cfeature.BORDERS, linewidth=0.5, zorder=2)\n",
    "    ax.add_feature(cfeature.LAND, color='darkgray', zorder=0)\n",
    "    ax.add_feature(cfeature.OCEAN, color='lightgray', zorder=1)\n",
    "    ax.gridlines(draw_labels=False, linewidth=0.5, color='gray', alpha=0.5)\n",
    "\n",
    "    # Plot without auto colorbar\n",
    "    im = to_plot.plot(ax=ax, norm=cbar_dict['norm'], cmap=cbar_dict['cmap'],\n",
    "                      add_colorbar=False)\n",
    "    ims.append(im)\n",
    "\n",
    "    # Optional hatching\n",
    "    if to_hatch is not None:\n",
    "        to_hatch.plot.contour(ax=ax, levels=[-0.5, 0.5],\n",
    "                              hatches=['....', None], colors='none',\n",
    "                              add_colorbar=False)\n",
    "\n",
    "    ax.set_title('(%s) %s' % (letters[ct], fig_names[ct]))\n",
    "\n",
    "# Add a colorbar axis below the whole figure\n",
    "cbar_ax = fig.add_axes([0.16, 0.05, 0.7, 0.02])  \n",
    "\n",
    "cb = fig.colorbar(ims[0], cax=cbar_ax, orientation='horizontal', extend=extend)\n",
    "cb.set_label('Signal-to-noise ratio', labelpad=0)\n",
    "cb.ax.tick_params(length=3)\n",
    "\n",
    "plt.savefig('%s/SUPP_S2N.png' % figdir, dpi=200, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "055a5b8f",
   "metadata": {},
   "source": [
    "## GOGA trends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18edbb85",
   "metadata": {},
   "outputs": [],
   "source": [
    "goga_idx = np.where(['%s_%s' % ('GOGA', this_var) == n for n in name_list])[0][0]\n",
    "goga_mean_trend = trend_maps[goga_idx].mean('member')\n",
    "\n",
    "name = 'GOGA SW$_\\downarrow$ trend (W/m$^2$/yr), %i-%i' % (start_year[goga_idx], end_year[goga_idx])\n",
    "plot_global_discrete(goga_mean_trend.sel(lat=slice(-60, 80)).rename(name), levels=trend_cbar['levels'])\n",
    "\n",
    "plt.savefig('%s/SUPP_GOGA_trends.png' % figdir, dpi=200, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0162e2c",
   "metadata": {},
   "source": [
    "## Comparison of GOGA and ERA5 for 1985-2015: trend patterns, ranks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb2cb0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate ERA5 and GOGA trends over this period\n",
    "cesm2_idx = np.where(['%s_%s' % ('CESM2', this_var) == n for n in name_list])[0][0]\n",
    "for period_ct in range(2):\n",
    "    if period_ct == 0:\n",
    "        goga_period = 1985, 2015\n",
    "    else:\n",
    "        goga_period = 1980, 2020\n",
    "\n",
    "    short_trends = []\n",
    "    for ct in range(3):\n",
    "        if ct == 0:\n",
    "            this_idx = goga_idx\n",
    "            name = 'GOGA'\n",
    "        elif ct == 1:\n",
    "            this_idx = era5_idx\n",
    "            name = 'ERA5'\n",
    "        else:\n",
    "            this_idx = cesm2_idx\n",
    "            name = 'CESM2'\n",
    "\n",
    "        da_anom = all_sw[this_idx].groupby('time.month') - all_sw[this_idx].groupby('time.month').mean()\n",
    "        ann_mean = my_xutils.compute_annual_mean_of_full_years(da_anom)\n",
    "        ann_mean = ann_mean.sel(year=slice('%04i' % goga_period[0], '%04i' % goga_period[1]))\n",
    "        trend = my_xutils.compute_linear_trend_per_year(ann_mean)\n",
    "        if name == 'CESM2':\n",
    "            trend = trend.rename({'member': 'LE_member'})\n",
    "        short_trends.append(trend.rename(name))\n",
    "\n",
    "    short_trends = xr.merge(short_trends)\n",
    "\n",
    "    rank_obs_with_goga = xr.apply_ufunc(\n",
    "            my_utils.rank_func,\n",
    "            short_trends['GOGA'],\n",
    "            short_trends['ERA5'],\n",
    "            input_core_dims=[['member'], []],\n",
    "            output_core_dims=[[]],\n",
    "            vectorize=True,\n",
    "            dask='parallelized',\n",
    "            output_dtypes=[float]\n",
    "        )\n",
    "\n",
    "    # Plot\n",
    "    # (a) ERA5 trend\n",
    "    # (b) GOGA trend\n",
    "    # (c) ERA5 rank in GOGA\n",
    "\n",
    "    fig_names = ('ERA5 (%04i-%04i)' % (goga_period[0], goga_period[1]),\n",
    "                 'CESM2-AMIP MMM (%04i-%04i)' % (goga_period[0], goga_period[1]),\n",
    "                 'Rank of ERA5 within CESM2-AMIP (%04i-%04i)' % (goga_period[0], goga_period[1]),\n",
    "                 'CESM2-LE MMM (%04i-%04i)' % (goga_period[0], goga_period[1]))\n",
    "    cbar_names = trend_str, trend_str, 'Rank', trend_str\n",
    "\n",
    "    nrows, ncols = 2, 2\n",
    "    fig, axes = plt.subplots(\n",
    "        nrows=nrows, ncols=ncols,\n",
    "        figsize=(7 * ncols, 3 * nrows),\n",
    "        subplot_kw={'projection': ccrs.PlateCarree()},\n",
    "        gridspec_kw={'hspace': 0.4, 'wspace': 0.15}\n",
    "    )\n",
    "\n",
    "    for ct, ax in enumerate(axes.flatten()):\n",
    "        if ct == 0:\n",
    "            to_plot = short_trends['ERA5']\n",
    "            to_hatch = quality_mask\n",
    "            cbar_dict = trend_cbar\n",
    "            extend = 'both'\n",
    "        elif ct == 1:\n",
    "            to_plot = short_trends['GOGA'].mean('member')\n",
    "            cbar_dict = trend_cbar\n",
    "            extend = 'both'\n",
    "            to_hatch = None\n",
    "        elif ct == 2:\n",
    "            to_plot = rank_obs_with_goga.where(quality_mask)\n",
    "            to_hatch = quality_mask\n",
    "            rank_min, rank_max = int(to_plot.min()), int(to_plot.max())\n",
    "            ranks_edge = int(np.ceil(0.025 * (rank_max)))\n",
    "            levels = np.hstack((np.arange(rank_min - 0.5, rank_min + ranks_edge),\n",
    "                                np.arange(rank_max + 0.5 - ranks_edge, rank_max + 1.5)))\n",
    "            levels = np.arange(0.5, 12, 1)\n",
    "            norm = mcolors.BoundaryNorm(levels, ncolors=256)\n",
    "            cbar_dict = {'levels': levels, 'norm': norm, 'cmap': 'RdBu_r'}\n",
    "            extend = 'neither'\n",
    "        else:\n",
    "            to_plot = short_trends['CESM2'].mean('LE_member')\n",
    "            cbar_dict = trend_cbar\n",
    "            extend = 'both'\n",
    "            to_hatch = None\n",
    "\n",
    "        to_plot = to_plot.where(analysis_mask)\n",
    "\n",
    "        # Map look\n",
    "        ax.set_extent([-180, 180, -60, 80])\n",
    "        ax.coastlines(zorder=3, lw=1)\n",
    "        ax.add_feature(cfeature.BORDERS, linewidth=0.5, zorder=2)\n",
    "        ax.add_feature(cfeature.LAND, color='darkgray', zorder=0)\n",
    "        ax.add_feature(cfeature.OCEAN, color='lightgray', zorder=1)\n",
    "        ax.gridlines(draw_labels=False, linewidth=0.5, color='gray', alpha=0.5)\n",
    "\n",
    "        im = to_plot.plot(ax=ax, norm=cbar_dict['norm'], cmap=cbar_dict['cmap'],\n",
    "                          add_colorbar=False)\n",
    "        divider = make_axes_locatable(ax)\n",
    "        cax = divider.append_axes('bottom', size='6%', pad=0.15, axes_class=plt.Axes)\n",
    "        cb = fig.colorbar(im, cax=cax, orientation='horizontal', extend=extend)\n",
    "        cb.set_label(cbar_names[ct], labelpad=0)\n",
    "        cb.ax.tick_params(length=3)\n",
    "        if ct == 2:\n",
    "            mid_bins = (levels[:-1] + 0.5).astype(int)\n",
    "            cb.set_ticks(mid_bins)\n",
    "\n",
    "        # Optional hatching\n",
    "        if to_hatch is not None:\n",
    "            to_hatch.plot.contourf(ax=ax, levels=[-0.5, 0.5, 1],\n",
    "                                  hatches=['....', None], colors='none',\n",
    "                                  add_colorbar=False, zorder=5)\n",
    "\n",
    "        ax.set_title('(%s) %s' % (letters[ct], fig_names[ct]))\n",
    "\n",
    "\n",
    "    plt.savefig('%s/SUPP_GOGA_comparison_%04i-%04i.png' % (figdir, goga_period[0], goga_period[1]), \n",
    "                dpi=200, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17d00adc",
   "metadata": {},
   "source": [
    "## Trend for model that looks most like ERA5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ad15f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "nrows = 3\n",
    "ncols = 1\n",
    "fig = plt.figure(figsize=(6 * ncols, 3.5 * nrows))\n",
    "proj = ccrs.PlateCarree()\n",
    "gs = gridspec.GridSpec(\n",
    "    nrows=nrows, ncols=ncols, figure=fig,\n",
    "    hspace=0.12, wspace=0.18)\n",
    "\n",
    "for ct in range(3):\n",
    "\n",
    "    ax = fig.add_subplot(gs[ct], projection=proj)\n",
    "    if ct == 0:\n",
    "        max_corr_with_era5 = np.argmax(rho_member_with_era5)\n",
    "        to_plot = all_members.isel(member=max_corr_with_era5)\n",
    "        name = to_plot.member.values\n",
    "    else:\n",
    "        min_corr_within_ensemble = np.min(rho_member_trends[~np.isnan(rho_member_trends)])\n",
    "        idx = np.where(rho_member_trends == min_corr_within_ensemble)\n",
    "        if ct == 1:\n",
    "            to_plot = all_members.isel(member=idx[0][0])\n",
    "        else:\n",
    "            to_plot = all_members.isel(member=idx[1][0])\n",
    "        name = to_plot.member.values\n",
    "                \n",
    "    to_plot = to_plot.where(analysis_mask)\n",
    "\n",
    "    cbar_dict = trend_cbar\n",
    "    extend = 'both'\n",
    "\n",
    "\n",
    "    # Map look\n",
    "    ax.set_extent([-180, 180, -60, 80])\n",
    "    ax.coastlines(zorder=3, lw=1)\n",
    "    ax.add_feature(cfeature.BORDERS, linewidth=0.5, zorder=2)\n",
    "    ax.add_feature(cfeature.LAND, color='darkgray', zorder=0)\n",
    "    ax.add_feature(cfeature.OCEAN, color='lightgray', zorder=1)\n",
    "    ax.gridlines(draw_labels=False, linewidth=0.5, color='gray', alpha=0.5)\n",
    "\n",
    "    # Plot without auto colorbar\n",
    "    im = to_plot.plot(ax=ax, norm=cbar_dict['norm'], cmap=cbar_dict['cmap'],\n",
    "                      add_colorbar=False)\n",
    "\n",
    "    # --- Horizontal colorbar under the map, same width ---\n",
    "    divider = make_axes_locatable(ax)\n",
    "    cax = divider.append_axes('bottom', size='6%', pad=0.15, axes_class=plt.Axes)\n",
    "    cb = fig.colorbar(im, cax=cax, orientation='horizontal', extend=extend)\n",
    "    cb.set_label('SW$_\\downarrow$ trend (W/m$^2$/year)', labelpad=0)\n",
    "    cb.ax.tick_params(length=3)\n",
    "\n",
    "    ax.set_title('(%s) %s trend' % (letters[ct], name))\n",
    "        \n",
    "plt.savefig('%s/SUPP_example_members.png' % figdir, dpi=200, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afc8369c",
   "metadata": {},
   "source": [
    "## Rank of obs within each ensemble for total clouds\n",
    "- CMIP6: percentage\n",
    "- CESM2: fraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38529d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CMIP6 trends for total cloud\n",
    "da_cmip_clt_beta = xr.open_dataarray('%s/CMIP6_clt_historical-ssp370_1980-2024.nc' % procdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "024b469e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Supplement with CESM2\n",
    "cloud_var = 'CLDTOT'\n",
    "files = sorted(glob('%s/%s/b.e21.B*smbb.f09_g17.*.*.h0.%s.??????-??????.nc' % \n",
    "                    (data_dirs['CESM2'], cloud_var, cloud_var)))\n",
    "ens_members = np.unique(np.array(['.'.join(f.split('.')[4:6]) for f in files]))\n",
    "\n",
    "files = [\n",
    "    f for f in files\n",
    "    if (years := my_utils.extract_years(f)) and (years[0] <= (years_to_use[-1] + 1) \n",
    "                                                 and years[1] >= (years_to_use[0] + 1))\n",
    "]\n",
    "\n",
    "da_clt_cesm2 = []\n",
    "\n",
    "# Load each ensemble member\n",
    "for ens_member in ens_members:\n",
    "    use_files = [f for f in files if ens_member in f]\n",
    "    da = xr.open_mfdataset(use_files)[cloud_var]\n",
    "\n",
    "\n",
    "    da = my_utils.regrid_to_shared_grid(da, shared_lats, shared_lons)\n",
    "    da_clt_cesm2.append(da.load())\n",
    "\n",
    "da_clt_cesm2 = xr.concat(da_clt_cesm2, dim='member')\n",
    "da_clt_cesm2['member'] = ens_members\n",
    "da_clt_cesm2 = da_clt_cesm2.rename(cloud_var)\n",
    "\n",
    "# Move time by one month because of CESM2 timestamp issues for monthly data\n",
    "# (the monthly averages are saved with the next month's time)\n",
    "new_time = [cftime.DatetimeNoLeap(t.year, t.month - 1, t.day) if t.month > 1 \n",
    "            else cftime.DatetimeNoLeap(t.year - 1, 12, t.day) \n",
    "            for t in da_clt_cesm2['time'].values]\n",
    "da_clt_cesm2['time'] = new_time\n",
    "da_clt_cesm2 = da_clt_cesm2.sel(time=slice('%04i' % years_to_use[0], '%04i' % years_to_use[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ada5cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "da_clt_cesm2_beta = my_xutils.compute_linear_trend_per_year(\n",
    "    my_xutils.compute_annual_mean_of_full_years(da_clt_cesm2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc933d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rank obs within CESM2 + CMIP6\n",
    "da_model_ensemble_tc = xr.concat((da_clt_cesm2_beta * 100,\n",
    "                                  da_cmip_clt_beta.rename({'model': 'member'}).drop('base_model')), \n",
    "                                 dim='member')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5123c6e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "rank_obs_with_ensemble_tc = xr.apply_ufunc(\n",
    "    my_utils.rank_func,\n",
    "    da_model_ensemble_tc,\n",
    "    beta_tc * 100,  # switch ERA5 to percentage as well \n",
    "    input_core_dims=[['member'], []],\n",
    "    output_core_dims=[[]],\n",
    "    vectorize=True,\n",
    "    dask='parallelized',\n",
    "    output_dtypes=[float]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e399dd1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_names = 'CMIP6 MMM', 'Rank of ERA5 within CMIP6+CESM2'\n",
    "cbar_names = trend_str, 'Rank'\n",
    "\n",
    "nrows = 1\n",
    "ncols = 1\n",
    "fig = plt.figure(figsize=(6 * ncols, 4 * nrows))\n",
    "proj = ccrs.PlateCarree()\n",
    "gs = gridspec.GridSpec(\n",
    "    nrows=nrows, ncols=ncols, figure=fig)\n",
    "\n",
    "ax = fig.add_subplot(gs[0], projection=proj)\n",
    "\n",
    "to_plot = rank_obs_with_ensemble_tc.where(analysis_mask).where(quality_mask)\n",
    "\n",
    "rank_min, rank_max = int(to_plot.min()), int(to_plot.max())\n",
    "ranks_edge = int(np.ceil(0.025 * (rank_max)))\n",
    "levels = np.hstack((np.arange(rank_min - 0.5, rank_min + ranks_edge),\n",
    "                    np.arange(rank_max + 0.5 - ranks_edge, rank_max + 1.5)))\n",
    "norm = mcolors.BoundaryNorm(levels, ncolors=256)\n",
    "cbar_dict = {'levels': levels, 'norm': norm, 'cmap': 'RdBu_r'}\n",
    "extend = 'neither'\n",
    "to_hatch = quality_mask\n",
    "\n",
    "# Map look\n",
    "ax.set_extent([-180, 180, -60, 80])\n",
    "ax.coastlines(zorder=3, lw=1)\n",
    "ax.add_feature(cfeature.BORDERS, linewidth=0.5, zorder=2)\n",
    "ax.add_feature(cfeature.LAND, color='darkgray', zorder=0)\n",
    "ax.add_feature(cfeature.OCEAN, color='lightgray', zorder=1)\n",
    "ax.gridlines(draw_labels=False, linewidth=0.5, color='gray', alpha=0.5)\n",
    "\n",
    "# Plot without auto colorbar\n",
    "im = to_plot.plot(ax=ax, norm=cbar_dict['norm'], cmap=cbar_dict['cmap'],\n",
    "                  add_colorbar=False)\n",
    "\n",
    "# --- Horizontal colorbar under the map, same width ---\n",
    "divider = make_axes_locatable(ax)\n",
    "cax = divider.append_axes('bottom', size='6%', pad=0.15, axes_class=plt.Axes)\n",
    "cb = fig.colorbar(im, cax=cax, orientation='horizontal', extend=extend)\n",
    "cb.set_label('Rank', labelpad=0)\n",
    "cb.ax.tick_params(length=3)\n",
    "\n",
    "mid_bins = np.hstack(((np.ceil(levels)[:ranks_edge]).astype(int), \n",
    "                      (np.ceil(levels)[-(ranks_edge + 1):-1]).astype(int)))\n",
    "cb.set_ticks(mid_bins)\n",
    "\n",
    "# Optional hatching\n",
    "\n",
    "to_hatch.plot.contourf(ax=ax, levels=[-0.5, 0.5],\n",
    "                       hatches=['....', None], colors='none',\n",
    "                       add_colorbar=False)\n",
    "ax.set_title('ERA5 rank within CMIP6+CESM2 total cloud trends')\n",
    "\n",
    "plt.savefig('%s/SUPP_total_cloud_ranks.png' % figdir, dpi=200, bbox_inches='tight')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diurnal",
   "language": "python",
   "name": "diurnal"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
